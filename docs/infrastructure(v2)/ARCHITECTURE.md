# Production Infrastructure Architecture

## System Overview

The GoodSeed Cannabis App uses a modern, serverless-first architecture designed for scalability, reliability, and cost-efficiency. This document outlines the complete system architecture for production deployment.

---

## ğŸ—ï¸ Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              END USERS                                    â”‚
â”‚  â€¢ Web Browsers (Desktop/Mobile)                                         â”‚
â”‚  â€¢ API Consumers                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â”‚ HTTPS/TLS 1.3
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         VERCEL EDGE NETWORK                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Global CDN (100+ Edge Locations)                                  â”‚  â”‚
â”‚  â”‚  â€¢ Automatic HTTPS/SSL                                              â”‚  â”‚
â”‚  â”‚  â€¢ DDoS Protection (Layer 3/4/7)                                    â”‚  â”‚
â”‚  â”‚  â€¢ Edge Caching (Static Assets, API Responses)                     â”‚  â”‚
â”‚  â”‚  â€¢ Geolocation Routing                                              â”‚  â”‚
â”‚  â”‚  â€¢ Web Application Firewall (WAF)                                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     VERCEL (Next.js Application)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  PRESENTATION LAYER (Next.js 16 - App Router)                       â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ Server Componentsâ”‚  â”‚  Client Componentsâ”‚   â”‚  Middleware     â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ SSR Pages      â”‚  â”‚  â€¢ Interactive UI â”‚   â”‚  â€¢ Auth Check   â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ SEO Optimized  â”‚  â”‚  â€¢ React Query    â”‚   â”‚  â€¢ Rate Limit   â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ Streaming      â”‚  â”‚  â€¢ State Mgmt     â”‚   â”‚  â€¢ Logging      â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  API LAYER (Serverless Functions)                                  â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚     â”‚
â”‚  â”‚  â”‚ /api/auth/*  â”‚  â”‚ /api/seeds/* â”‚  â”‚ /api/scraper/*         â”‚    â”‚     â”‚ 
â”‚  â”‚  â”‚ Authenticationâ”‚  â”‚ Product CRUD â”‚  â”‚ Job Management         â”‚   â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚     â”‚
â”‚  â”‚  â”‚ /api/cron/*  â”‚  â”‚ /api/admin/* â”‚  â”‚ /api/webhooks/*        â”‚    â”‚     â”‚
â”‚  â”‚  â”‚ Scheduled    â”‚  â”‚ Admin Panel  â”‚  â”‚ External Integrations  â”‚    â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  EDGE FUNCTIONS (Ultra-low latency)                                  â”‚   â”‚
â”‚  â”‚  â€¢ Geolocation-based content                                         â”‚   â”‚
â”‚  â”‚  â€¢ A/B Testing                                                       â”‚   â”‚
â”‚  â”‚  â€¢ Feature Flags                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚          â”‚             â”‚              â”‚             â”‚
     â”‚          â”‚             â”‚              â”‚             â”‚
     â–¼          â–¼             â–¼              â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NEON   â”‚ â”‚ UPSTASH  â”‚ â”‚ RESEND   â”‚ â”‚  GITHUB    â”‚ â”‚   SENTRY     â”‚
â”‚  (DB)   â”‚ â”‚ (Redis)  â”‚ â”‚ (Email)  â”‚ â”‚  ACTIONS   â”‚ â”‚  (Errors)    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚           â”‚            â”‚             â”‚
     â”‚           â”‚            â”‚             â”‚ Cron Trigger (Cleanup Only)
     â”‚           â”‚            â”‚             â”‚ â€¢ Weekly Sunday 2 AM UTC
     â”‚           â”‚            â”‚             â–¼
     â”‚           â”‚            â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚           â”‚            â”‚      â”‚  /api/cron/cleanup-*      â”‚
     â”‚           â”‚            â”‚      â”‚  (Vercel API Routes)      â”‚
     â”‚           â”‚            â”‚      â”‚  â€¢ cleanup-stuck-jobs     â”‚
     â”‚           â”‚            â”‚      â”‚  â€¢ cleanup-rate-limits    â”‚
     â”‚           â”‚            â”‚      â”‚  (Short-term tasks only)  â”‚
     â”‚           â”‚            â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚           â”‚            â”‚
     â”‚           â–¼            â”‚                 
     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚                 
     â”‚    â”‚  BULL QUEUE  â”‚â—„â”€â”€â”€â”¼â”€â”€â”€ Dashboard RUN Button (PRIMARY)
     â”‚    â”‚  (Upstash)   â”‚    â”‚    Admin schedules repeatable jobs
     â”‚    â”‚              â”‚    â”‚    POST /api/admin/scraper/schedule-all
     â”‚    â”‚  Job Types:  â”‚    â”‚
     â”‚    â”‚  â€¢ Scraping  â”‚    â”‚
     â”‚    â”‚  â€¢ Price     â”‚    â”‚
     â”‚    â”‚    Detection â”‚    â”‚
     â”‚    â”‚  â€¢ Email     â”‚    â”‚
     â”‚    â”‚  â€¢ Reports   â”‚    â”‚
     â”‚    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
     â”‚           â”‚            â”‚
     â”‚           â”‚ Pick Jobs  â”‚
     â”‚           â–¼            â”‚
     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚    â”‚   RENDER WORKER SERVICE (Background Worker)     â”‚
     â”‚    â”‚   Docker Container - Always Running             â”‚
     â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
     â”‚    â”‚  â”‚  Scraper Worker (lib/workers/scraper-    â”‚   â”‚
     â”‚    â”‚  â”‚  worker.ts)                              â”‚   â”‚
     â”‚    â”‚  â”‚  â€¢ Pick jobs from Bull Queue             â”‚   â”‚
     â”‚    â”‚  â”‚  â€¢ Process with Crawlee/Cheerio          â”‚   â”‚
     â”‚    â”‚  â”‚  â€¢ Normalize & validate data             â”‚   â”‚
     â”‚    â”‚  â”‚  â€¢ Save to Neon DB (Pricing + History)   â”‚   â”‚
     â”‚    â”‚  â”‚  â€¢ Detect price changes (â‰¥5% drops)      â”‚   â”‚
     â”‚    â”‚  â”‚  â€¢ Find users with wishlist matches      â”‚   â”‚
     â”‚    â”‚  â”‚  â€¢ Send price alert emails               â”‚   â”‚
     â”‚    â”‚  â”‚  â€¢ Update job status                     â”‚   â”‚
     â”‚    â”‚  â”‚  â€¢ Error handling & retry logic          â”‚   â”‚
     â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
     â”‚    â”‚                     â”‚                           â”‚
     â”‚    â”‚  Health Endpoint:   â”‚                           â”‚
     â”‚    â”‚  GET /health â†’ 200 OK                           â”‚
     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                          â”‚
     â”‚                          â”‚ Scrape
     â”‚                          â–¼
     â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                   â”‚  EXTERNAL SITES  â”‚
     â”‚                   â”‚  â€¢ Seed Banks    â”‚
     â”‚                   â”‚  â€¢ Product Data  â”‚
     â”‚                   â”‚  â€¢ Pricing Info  â”‚
     â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      NEON POSTGRESQL DATABASE         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Production Database            â”‚  â”‚
â”‚  â”‚  â€¢ Auto-scaling                 â”‚  â”‚
â”‚  â”‚  â€¢ Connection Pooling           â”‚  â”‚
â”‚  â”‚  â€¢ Point-in-time Recovery       â”‚  â”‚
â”‚  â”‚  â€¢ Automatic Backups            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Database Branching             â”‚  â”‚
â”‚  â”‚  â€¢ main (production)            â”‚  â”‚
â”‚  â”‚  â€¢ preview-* (PR environments)  â”‚  â”‚
â”‚  â”‚  â€¢ dev (development)            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Component Details

### 1. Vercel Platform

**Purpose**: Application hosting, serverless functions, edge network

**Key Features**:
- **Global Edge Network**: 100+ edge locations worldwide
- **Automatic Scaling**: Handles traffic spikes automatically
- **Zero-Config Deployments**: Push to deploy
- **Preview Deployments**: Every PR gets a unique URL
- **Built-in Analytics**: Web Vitals, Core Web Vitals
- **Edge Functions**: Ultra-low latency compute at the edge

**Configuration**:
```json
{
  "framework": "nextjs",
  "buildCommand": "pnpm build",
  "devCommand": "pnpm dev",
  "installCommand": "pnpm install",
  "outputDirectory": ".next"
}
```

**Serverless Function Limits**:
- **Hobby**: 10s timeout, 1024MB memory
- **Pro**: 60s timeout, 3008MB memory
- **Enterprise**: 900s timeout, 3008MB memory

---

### 2. Neon PostgreSQL

**Purpose**: Primary application database

**Key Features**:
- **Serverless Architecture**: Pay only for compute used
- **Instant Database Branching**: Create database copies in seconds
- **Connection Pooling**: Built-in PgBouncer
- **Point-in-Time Recovery**: Restore to any point in the last 7-30 days
- **Autoscaling**: Automatically scales compute based on demand
- **Autosuspend**: Pauses compute during inactivity (saves costs)

**Database Schema**:
```
Tables:
â”œâ”€â”€ User (authentication & preferences)
â”‚   â””â”€â”€ receivePriceAlerts (boolean) - opt-in for price alerts
â”œâ”€â”€ Account (OAuth providers)
â”œâ”€â”€ Session (user sessions)
â”œâ”€â”€ Seller (seed bank vendors)
â”œâ”€â”€ SeedProduct (cannabis seeds)
â”œâ”€â”€ Pricing (current product prices) â­
â”œâ”€â”€ PricingHistory (historical price records) â­
â”œâ”€â”€ Wishlist (user favorite products) â­
â”œâ”€â”€ WishlistFolder (user wishlist organization)
â”œâ”€â”€ ScrapeJob (scraping job tracking)
â”œâ”€â”€ ScrapingSource (data source config)
â”œâ”€â”€ Notification (user alerts)
â”œâ”€â”€ ContentPage (CMS content)
â””â”€â”€ FAQ (help content)

â­ New tables for Price Alert System
```

**Connection Configuration**:
```typescript
// Pooled connection (recommended)
DATABASE_URL="postgresql://user:pass@ep-xxx.aws.neon.tech/db?sslmode=require"

// Direct connection (for migrations)
DIRECT_URL="postgresql://user:pass@ep-xxx.aws.neon.tech/db?sslmode=require"
```

---

### 3. Upstash Redis

**Purpose**: In-memory cache, job queue, session storage

**Key Features**:
- **Serverless Redis**: Pay-per-request pricing
- **Global Replication**: Multi-region support
- **REST API**: HTTP-based access (no connection pooling issues)
- **Durable Storage**: Data persisted to disk
- **Built-in Metrics**: Monitor usage in real-time

**Use Cases**:

1. **Bull Queue (Job Scheduling)**:
```typescript
// Job types
- Scraping jobs (scheduled/manual)
- Email sending (transactional/bulk)
- Database cleanup
- Report generation
```

2. **Application Caching**:
```typescript
// Cached data
- Product listings (5 min TTL)
- Seller information (1 hour TTL)
- Search results (10 min TTL)
- API responses (configurable)
```

3. **Rate Limiting**:
```typescript
// Limits
- API: 100 requests/minute per IP
- Scraping: 30 jobs/minute per seller
- Email: 10 emails/minute per user
```

4. **Session Storage**:
```typescript
// NextAuth sessions
- User sessions
- OAuth states
- CSRF tokens
```

**Configuration**:
```typescript
{
  redis: {
    host: process.env.REDIS_HOST,
    port: parseInt(process.env.REDIS_PORT),
    password: process.env.REDIS_PASSWORD,
    tls: {}, // Required for Upstash
    maxRetriesPerRequest: 3,
    enableReadyCheck: true
  }
}
```

---

### 4. Resend Email Service

**Purpose**: Transactional email delivery

**Key Features**:
- **99.9% Uptime SLA**: Reliable email delivery
- **Developer-First API**: Simple, modern API
- **Email Templates**: React-based email templates
- **Domain Verification**: Send from your own domain
- **Analytics**: Open rates, click tracking, bounces
- **Webhooks**: Real-time delivery notifications

**Email Types**:

1. **Authentication Emails**:
   - Email verification
   - Password reset
   - Magic link login
   - Account activation

2. **Notification Emails**:
   - Scraping job completed
   - Scraping job failed
   - Price drop alerts â­ NEW
   - New products available
   - User wishlist updates

3. **System Emails**:
   - Admin alerts
   - Error notifications
   - Weekly reports
   - User feedback

**Configuration**:
```typescript
import { Resend } from 'resend';

const resend = new Resend(process.env.RESEND_API_KEY);

await resend.emails.send({
  from: 'GoodSeed <noreply@lembooking.com>',
  to: user.email,
  subject: 'Welcome to GoodSeed',
  react: WelcomeEmail({ name: user.name })
});
```

---

### 5. Render Worker Service

**Purpose**: Process long-running scraping jobs from Bull Queue

**âš ï¸ CRITICAL REQUIREMENT: Render Starter Plan ($7/month) is REQUIRED for production**

**Why Starter Plan is Mandatory:**
- âœ… **Always-On**: Worker runs 24/7, no auto-sleep
- âœ… **Instant Processing**: No cold start delays (30s on free tier)
- âœ… **Scheduled Jobs**: Dashboard RUN button creates repeatable jobs that require always-on worker
- âœ… **Reliable**: Free tier auto-sleeps after 15 minutes â†’ jobs stuck in queue
- âŒ **Free Tier NOT Supported**: Auto-sleep breaks Bull Queue repeatable jobs

**Key Features**:
- **Always-On Workers**: No cold starts, always ready to process jobs
- **Docker Support**: Custom environment with Chromium for scraping
- **Auto-Deploy**: Deploys automatically from GitHub on push
- **Health Monitoring**: Built-in health checks and auto-restart
- **Persistent Disk**: Optional disk storage for large files

**Pricing Tiers**:
```
âŒ Free Tier ($0/month):
- 750 hours/month
- Auto-sleep after 15 min inactivity
- NOT SUITABLE: Breaks scheduled jobs, requires wake-up mechanism
- Use only for: Development/testing

âœ… Starter ($7/month) - REQUIRED FOR PRODUCTION:
- Always-on, no sleep
- 512MB RAM
- Shared CPU
- Perfect for: Production workloads with scheduled jobs

Standard ($25/month):
- 2GB RAM
- Faster processing
- Suitable for: Medium traffic

Pro ($85/month):
- 4GB RAM
- Dedicated CPU
- Auto-scaling
- Suitable for: High traffic, enterprise
```

**Worker Implementation**:
```typescript
// lib/workers/scraper-worker.ts
import { scraperQueue } from '@/lib/queue/scraper-queue';
import { apiLogger } from '@/lib/helpers/api-logger';
import express from 'express';

// Health check HTTP server (required for Render monitoring)
const app = express();
const PORT = process.env.PORT || 3001;

app.get('/health', async (req, res) => {
  const queueStats = {
    waiting: await scraperQueue.getWaitingCount(),
    active: await scraperQueue.getActiveCount(),
    completed: await scraperQueue.getCompletedCount(),
    failed: await scraperQueue.getFailedCount()
  };
  
  res.status(200).json({ 
    status: 'healthy',
    uptime: process.uptime(),
    queue: queueStats,
    timestamp: new Date().toISOString()
  });
});

// Start HTTP server for health checks
app.listen(PORT, () => {
  apiLogger.info(`[Worker] Health endpoint running on port ${PORT}`);
});

// Process jobs from Bull Queue (always running on Starter plan)
scraperQueue.process(async (job) => {
  const { jobId, sellerId, scrapingSources } = job.data;
  
  apiLogger.info(`[Worker] Processing job ${jobId} for seller ${sellerId}`);
  
  try {
    // 1. Update job status to PROCESSING
    await updateJobStatus(jobId, 'PROCESSING');
    
    // 2. Run scraper with Crawlee
    const scrapedData = await runCrawlee({
      sellerId,
      sources: scrapingSources
    });
    
    // 3. Normalize and validate data
    const normalizedData = normalizeProducts(scrapedData);
    
    // 4. Save to Neon database
    // â­ CRITICAL: Save OLD prices to PricingHistory BEFORE updating Pricing
    const saveResult = await saveProductsToDatabase(normalizedData);
    
    // 5. Populate seedIds for price detection
    const seedIds = await populateSeedIds(normalizedData, sellerId);
    
    // 6. â­ NEW: Detect price changes
    const priceChanges = await detectPriceChanges(seedIds);
    apiLogger.info(`[Worker] Detected ${priceChanges.length} price drops â‰¥5%`);
    
    // 7. â­ NEW: Find users to notify
    if (priceChanges.length > 0) {
      const usersToNotify = await findUsersToNotify(priceChanges);
      apiLogger.info(`[Worker] Found ${usersToNotify.length} users to notify`);
      
      // 8. â­ NEW: Send price alert emails
      for (const userNotification of usersToNotify) {
        await sendPriceAlertEmail(userNotification);
      }
    }
    
    // 9. Update job status to COMPLETED
    await updateJobStatus(jobId, 'COMPLETED', {
      productsScraped: normalizedData.length,
      priceChangesDetected: priceChanges.length,
      emailsSent: usersToNotify?.length || 0,
      completedAt: new Date()
    });
    
    // 10. Send job completion notification to admin
    await sendJobCompleteEmail(jobId, sellerId);
    
    apiLogger.info(`[Worker] Job ${jobId} completed successfully`);
    
  } catch (error) {
    apiLogger.logError(`[Worker] Job ${jobId} failed`, error);
    
    // Update job status to FAILED
    await updateJobStatus(jobId, 'FAILED', {
      error: error.message
    });
    
    // Bull will retry based on job options
    throw error;
  }
});

apiLogger.info('[Worker] Scraper worker started and listening for jobs');
```

**Dockerfile Configuration**:
```dockerfile
FROM node:20-alpine

# Install Chromium for Crawlee/Puppeteer
RUN apk add --no-cache \
    chromium \
    nss \
    freetype \
    harfbuzz \
    ca-certificates \
    ttf-freefont

# Set Puppeteer to use system Chromium
ENV PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser
ENV PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=true

WORKDIR /app

# Install dependencies
COPY package.json pnpm-lock.yaml ./
RUN npm install -g pnpm && pnpm install --frozen-lockfile

# Copy application code
COPY . .

# Generate Prisma Client
RUN npx prisma generate

# Expose health check port
EXPOSE 3001

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
  CMD node -e "require('http').get('http://localhost:3001/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1))"

# Start worker
CMD ["pnpm", "run", "worker:scraper"]
```

**Deployment Steps**:
1. Create Render account (free signup)
2. Connect GitHub repository
3. Create "Background Worker" service
4. **Select Starter Plan ($7/month)** - REQUIRED
5. Configure environment variables (same as Vercel)
6. Select Dockerfile deployment
7. Deploy and verify health endpoint

**Monitoring**:
- **Health Checks**: Automatic via `/health` endpoint every 30 seconds
- **Logs**: Real-time logs in Render dashboard
- **Metrics**: CPU, memory, network usage
- **Alerts**: Email/Slack notifications on failures
- **Uptime**: 99.9% SLA on Starter plan and above

**Integration with Dashboard**:
```typescript
// Admin Dashboard â†’ RUN Button Flow
// app/dashboard/(components)/AutoScraperTabContent.tsx

Admin clicks RUN button
         â†“
POST /api/admin/scraper/schedule-all
         â†“
AutoScraperScheduler.initializeAllAutoJobs()
         â†“
Query sellers: WHERE isActive=true AND autoScrapeInterval > 0
         â†“
For each eligible seller:
  - Create repeatable job in Bull Queue (Upstash Redis)
  - Schedule pattern: `0 */${autoScrapeInterval} * * *`
  - Example: Seller A (6h), Seller B (24h)
         â†“
Render Worker (ALWAYS RUNNING on Starter plan):
  - Picks up jobs immediately from queue
  - Processes according to schedule
  - Repeats automatically every interval
  - No sleep, no wake-up needed
```

**Why Dashboard RUN is Primary Method**:
- âœ… Admin full control over scheduling
- âœ… Per-seller interval configuration (6h, 24h, etc.)
- âœ… Repeatable jobs work perfectly with always-on worker
- âœ… Instant job processing (no cold start)
- âœ… Real-time monitoring in dashboard
- âœ… Easy to start/stop individual sellers or all at once

---

### 6. GitHub Actions (Cron Scheduler)

**Purpose**: Scheduled cleanup and monitoring tasks (NOT for scraping trigger)

**âš ï¸ NOTE: Scraping is controlled via Dashboard RUN button, not GitHub Actions**

**Key Features**:
- **100% Free**: Unlimited for public repositories
- **Reliable Scheduling**: Industry-standard cron syntax
- **Manual Triggers**: Can trigger workflows manually
- **Secrets Management**: Secure environment variables
- **Workflow History**: Full audit log of all runs

**Configuration** (`.github/workflows/cron-jobs.yml`):
```yaml
name: Cleanup & Monitoring Jobs
on:
  schedule:
    - cron: '0 2 * * 0'  # Weekly on Sunday at 2 AM UTC
  workflow_dispatch:  # Manual trigger

jobs:
  cleanup-stuck-jobs:
    name: Cleanup Stuck Scraping Jobs
    runs-on: ubuntu-latest
    steps:
      - name: Cleanup Jobs Older Than 30 Minutes
        run: |
          curl -X GET "${{ secrets.APP_URL }}/api/cron/cleanup-stuck-jobs" \
            -H "Authorization: Bearer ${{ secrets.CRON_SECRET }}"
      
  cleanup-rate-limits:
    name: Cleanup Old Rate Limit Records
    runs-on: ubuntu-latest
    steps:
      - name: Cleanup Records Older Than 7 Days
        run: |
          curl -X GET "${{ secrets.APP_URL }}/api/cron/cleanup-rate-limits" \
            -H "Authorization: Bearer ${{ secrets.CRON_SECRET }}"
```

**Secrets Setup**:
1. Go to GitHub repository â†’ Settings â†’ Secrets and variables â†’ Actions
2. Add secrets:
   - `APP_URL`: `https://your-app.vercel.app`
   - `CRON_SECRET`: Your CRON_SECRET from .env.production

**Jobs Handled by GitHub Actions**:
- âœ… **Cleanup Stuck Jobs**: Remove jobs stuck in WAITING status
- âœ… **Cleanup Rate Limits**: Remove old rate limit records
- âœ… **Health Monitoring**: Optional health checks (can be added)
- âŒ **Scraping Trigger**: NOT handled here (use Dashboard RUN button)

**Why NOT Use GitHub Actions for Scraping:**
- âŒ Less flexible than dashboard control
- âŒ Admin can't adjust schedules easily
- âŒ Can't control per-seller intervals
- âœ… Dashboard RUN button is primary method with Render Starter

---

### 7. Background Workers (Legacy - Deprecated)
```typescript
// API Route: /api/scraper/process
export async function POST(request: Request) {
  const job = await scraperQueue.add({
    jobId: crypto.randomUUID(),
    sellerId: '123',
    scrapingSources: [...]
  });
  
  return Response.json({ jobId: job.id });
}

// Worker processes jobs from queue
scraperQueue.process(async (job) => {
  const result = await scrapeSeeds(job.data);
  await saveToDatabase(result);
});
```

**Limitations**:
- 10s timeout (Hobby), 60s (Pro)
- Not suitable for heavy scraping
- Cold starts

#### Option B: Dedicated Worker Service (Recommended for Production)

**Options**:
1. **Railway** ($5-10/month)
   - Always-on workers
   - Dockerfile support
   - Auto-deploy from GitHub
   
2. **Render** ($7/month)
   - Background workers
   - Free tier available (with sleep)
   
3. **Fly.io** (Pay-as-you-go)
   - Global distribution
   - Persistent storage

**Worker Architecture**:
```typescript
// lib/workers/scraper-worker.ts
import { scraperQueue } from '@/lib/queue/scraper-queue';

scraperQueue.process(async (job) => {
  apiLogger.info(`Processing job ${job.id}`);
  
  try {
    // Scrape data
    const data = await scraper.run(job.data);
    
    // Normalize and save
    await db.products.upsertMany(data);
    
    // Update job status
    await db.scrapeJob.update({
      where: { jobId: job.data.jobId },
      data: { status: 'COMPLETED' }
    });
    
    // Send notification
    await emailService.sendJobComplete(job.data);
    
  } catch (error) {
    apiLogger.logError('Job failed', { error });
    throw error; // Bull will retry
  }
});
```

---

### 6. Vercel Cron Jobs

**Purpose**: Schedule recurring tasks

**Configuration** (`vercel.json`):
```json
{
  "crons": [
    {
      "path": "/api/cron/scraper",
      "schedule": "0 2 * * *"
    },
    {
      "path": "/api/cron/cleanup-jobs",
      "schedule": "*/30 * * * *"
    },
    {
      "path": "/api/cron/send-emails",
      "schedule": "*/5 * * * *"
    }
  ]
}
```

**Cron Jobs**:

1. **Daily Scraping** (2 AM daily)
   - Scrape priority seed banks
   - Update product pricing
   - Check product availability

2. **Job Cleanup** (Every 30 min)
   - Remove completed jobs (>7 days old)
   - Retry failed jobs
   - Clear stuck jobs

3. **Email Queue** (Every 5 min)
   - Process email queue
   - Send pending notifications
   - Handle retries

4. **Database Maintenance** (Weekly)
   - Vacuum database
   - Update statistics
   - Archive old data

---

### 7. Monitoring & Observability

**Recommended Tools**:

#### Sentry (Error Tracking)
```typescript
import * as Sentry from '@sentry/nextjs';

Sentry.init({
  dsn: process.env.SENTRY_DSN,
  environment: process.env.NODE_ENV,
  tracesSampleRate: 0.1,
});
```

**Features**:
- Error tracking and grouping
- Performance monitoring
- Release tracking
- User feedback collection

#### Vercel Analytics
- Web Vitals monitoring
- Real User Monitoring (RUM)
- Page performance insights
- Traffic analytics

#### Custom Logging (apiLogger)
```typescript
// lib/helpers/api-logger.ts
export const apiLogger = {
  info: (message, meta) => console.log(JSON.stringify({...})),
  warn: (message, meta) => console.warn(JSON.stringify({...})),
  logError: (message, meta) => console.error(JSON.stringify({...}))
};
```

---

## ğŸ” Security Architecture

### Authentication Flow
```
User Login Request
    â†“
NextAuth.js Middleware
    â†“
Provider Authentication (Google/Facebook/Email)
    â†“
Create/Update Session (Prisma Adapter)
    â†“
Store Session in Database
    â†“
Set Secure HTTP-Only Cookie
    â†“
Return to Application
```

### Authorization Layers
1. **Route Protection**: Middleware checks auth status
2. **API Authorization**: Role-based access control (RBAC)
3. **Database Security**: Row-level security with Prisma
4. **Rate Limiting**: Upstash-based rate limiting

### Data Encryption
- **In Transit**: TLS 1.3 (Vercel automatic)
- **At Rest**: AES-256 (Neon automatic)
- **Secrets**: Vercel encrypted environment variables

---

## ğŸ“Š Data Flow Examples

### 1. User Browsing Products
```
User â†’ Edge CDN (cache hit?) â†’ Return cached HTML
     â†“ (cache miss)
     Next.js SSR â†’ Neon DB â†’ Render & Cache â†’ Return to user
```

### 2. Admin Triggers Scraping
```
Admin UI â†’ API /api/scraper/trigger
         â†“
    Add job to Bull Queue (Upstash Redis)
         â†“
    Worker picks up job
         â†“
    Scrape external site (Crawlee)
         â†“
    Normalize data
         â†“
    Save to Neon DB
         â†“
    Send completion email (Resend)
         â†“
    Update job status
```

### 3. Scheduled Daily Scraping
```
Vercel Cron (2 AM) â†’ Trigger /api/cron/scraper
                   â†“
              For each seller:
                   â†“
              Add scraping job to queue
                   â†“
              Worker processes jobs
                   â†“
              Update database
                   â†“
              Send summary email
```

---

## ğŸ¯ Performance Optimization

### Edge Caching Strategy
```typescript
// Static pages: 1 hour
export const revalidate = 3600;

// Product listings: 5 minutes
export const revalidate = 300;

// Dynamic content: No cache
export const revalidate = 0;
```

### Database Query Optimization
- **Indexes**: On frequently queried columns
- **Connection Pooling**: Prevent connection exhaustion
- **Query Batching**: Reduce round trips
- **Selective Fetching**: Only fetch needed fields

### Redis Caching Strategy
```typescript
// Cache hierarchy
1. Edge Cache (Vercel) - Static assets
2. Redis Cache (Upstash) - API responses
3. Database (Neon) - Source of truth
```

---

## ğŸ”„ Disaster Recovery

### Backup Strategy
- **Database**: Neon automatic daily backups (7-30 day retention)
- **Code**: GitHub repository (with version history)
- **Environment**: Vercel environment variables (encrypted)
- **Redis**: Upstash automatic persistence

### Recovery Procedures
1. **Application Failure**: Rollback deployment in Vercel (1-click)
2. **Database Corruption**: Point-in-time recovery via Neon
3. **Data Loss**: Restore from latest backup
4. **Service Outage**: Automatic failover (provider-managed)

---

## ğŸ“ˆ Scaling Strategy

### Horizontal Scaling
- **Application**: Automatic (Vercel serverless)
- **Database**: Connection pooling + read replicas
- **Workers**: Add more worker instances
- **Cache**: Upstash global replication

### Vertical Scaling
- **Database**: Upgrade Neon compute tier
- **Functions**: Upgrade Vercel plan (longer timeouts)
- **Redis**: Upgrade Upstash tier (more storage)

---

## ğŸ“ Summary

This architecture provides:
- âœ… **Scalability**: Auto-scales from 0 to millions of requests
- âœ… **Reliability**: 99.9%+ uptime across all services
- âœ… **Performance**: Edge caching, optimized queries
- âœ… **Security**: End-to-end encryption, role-based access
- âœ… **Observability**: Comprehensive logging and monitoring
- âœ… **Cost-Efficiency**: Pay only for what you use
- âœ… **Developer Experience**: Simple deployment, easy debugging

**Next Steps**: See [DEPLOYMENT-GUIDE.md](./DEPLOYMENT-GUIDE.md) for implementation details.
