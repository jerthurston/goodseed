# Scaling Guide: From Free Tier to Production

## Overview

This guide provides a comprehensive roadmap for scaling your GoodSeed Cannabis App infrastructure from free tier (demo/MVP) to paid production tiers as your application grows.

**Target Audience**: Technical teams planning production deployment or scaling existing applications

---

## Quick Comparison: Scaling Tiers

| Criteria | **Phase 1: Free Tier (MVP)** | **Phase 2: Starter Production** | **Phase 3: Growth Production** | **Phase 4: Enterprise** |
|----------|------------------------------|----------------------------------|--------------------------------|------------------------|
| **Monthly Cost** | **$0** | **~$102** | **~$368** | **$1,000+** |
| **Timeline** | 0-3 months | 3-6 months | 6-12 months | 12+ months |
| **Active Users** | 0-100 | 100-1,000 | 1,000-10,000 | 10,000+ |
| **Traffic** | <10K pageviews/month | 10K-100K pageviews/month | 100K-1M pageviews/month | 1M+ pageviews/month |
| **Database Size** | 0.5GB max | Up to 10GB | Up to 50GB | 200GB+ |
| **Concurrent Users** | 10-20 | 50-200 | 500-2,000 | 5,000+ |
| **API Requests** | <100K/month | 100K-1M/month | 1M-10M/month | 10M+/month |
| **Email Volume** | 100/day, 3K/month | 1,666/day, 50K/month | 33K/day, 1M/month | 333K/day, 10M+/month |
| **Scraping Capacity** | 500 products/hour | 5,000 products/hour | 10,000+ products/hour | Unlimited |
| **Function Timeout** | 10s | 60s | 60s | 300s |
| **Uptime SLA** | No SLA (99%+) | No SLA (99.9%+) | 99.99% available | 99.99% SLA |
| **Support** | Community | Email support | Priority support | Dedicated account team |
| **Monitoring** | Basic (Sentry free) | Standard (Sentry Team) | Advanced (Sentry Business) | Enterprise observability |
| **Backup Retention** | 7 days | 30 days | 30 days | 90 days |
| **Multi-Region** | âŒ Single region | âŒ Single region | âœ… Optional | âœ… Required |
| **Read Replicas** | âŒ No | âŒ No | âœ… Yes | âœ… Multiple |
| **Dedicated Resources** | âŒ Shared | âš ï¸ Partially dedicated | âœ… Mostly dedicated | âœ… Fully dedicated |
| **Auto-scaling** | âŒ Limited | âœ… Basic | âœ… Advanced | âœ… Full control |
| **Best For** | Proof of concept, demos, learning | MVP launch, early customers | Growing user base, established product | Enterprise, mission-critical |

### When to Upgrade

**Free â†’ Starter Production**:
- âœ… When you get first paying customers
- âœ… Database approaching 0.5GB limit
- âœ… Need automated scraping (cron jobs)
- âœ… Sending >100 emails/day
- âœ… Function timeouts occurring

**Starter â†’ Growth Production**:
- âœ… Database >10GB
- âœ… >1,000 active users
- âœ… Need high availability
- âœ… Require faster response times
- âœ… Need advanced monitoring

**Growth â†’ Enterprise**:
- âœ… >10,000 active users
- âœ… Need SLA guarantees
- âœ… Require multi-region deployment
- âœ… Compliance requirements
- âœ… Need dedicated support

---

## Table of Contents

1. [When to Scale](#when-to-scale)
2. [Scaling Phases](#scaling-phases)
3. [Service-by-Service Scaling](#service-by-service-scaling)
4. [Migration Procedures](#migration-procedures)
5. [Performance Benchmarks](#performance-benchmarks)
6. [Cost Optimization](#cost-optimization)
7. [Scaling Checklist](#scaling-checklist)

---

## When to Scale

### Indicators You Need to Upgrade

#### **Vercel**
```
Free Tier Limits Hit:
â–¡ Exceeding 100GB bandwidth/month
â–¡ Function execution timeout (10s) causing failures
â–¡ Need for cron jobs (not available on Hobby tier)
â–¡ Requiring >100 hours function execution/month
â–¡ Need for team collaboration features
â–¡ Preview deployments taking too long

Symptoms:
- Users experiencing slow page loads
- API requests timing out
- Build failures due to time limits
- Cannot schedule automated tasks
```

#### **Neon PostgreSQL**
```
Free Tier Limits Hit:
â–¡ Database size approaching 0.5GB
â–¡ Compute hours exceeding 191.9 hours/month
â–¡ Autosuspend delays causing cold starts
â–¡ Need for longer query timeout
â–¡ Requiring more concurrent connections
â–¡ Need for read replicas

Symptoms:
- "Database storage limit reached" errors
- Slow query performance
- Connection pool exhaustion
- Frequent cold starts (>30s delay)
- Need for point-in-time recovery beyond 7 days
```

#### **Upstash Redis**
```
Free Tier Limits Hit:
â–¡ Exceeding 10,000 commands/day (~7/min)
â–¡ Need for more than 256MB storage
â–¡ Requiring >100 concurrent connections
â–¡ Need for global replication
â–¡ Rate limiting becoming bottleneck

Symptoms:
- "Daily command limit exceeded" errors
- Queue jobs piling up
- Cache misses increasing
- Cannot process all background jobs
- Redis connection errors during peak hours
```

#### **Resend Email**
```
Free Tier Limits Hit:
â–¡ Sending >100 emails/day or >3,000/month
â–¡ Need for dedicated IP
â–¡ Requiring better deliverability
â–¡ Need for email API rate limits increase
â–¡ Advanced analytics required

Symptoms:
- "Email quota exceeded" errors
- Emails queued but not sending
- Users not receiving notifications
- Email delivery delays
```

---

## Scaling Phases

### Phase 1: Free Tier (MVP/Demo)
**Cost**: $0/month  
**Timeline**: 0-3 months  
**Users**: 0-100 active users  
**Traffic**: <10K pageviews/month

```
Infrastructure:
â”œâ”€â”€ Vercel Hobby (Free)
â”‚   â”œâ”€â”€ 100GB bandwidth
â”‚   â”œâ”€â”€ 100hrs function execution
â”‚   â””â”€â”€ 10s function timeout
â”œâ”€â”€ Neon Free
â”‚   â”œâ”€â”€ 0.5GB storage
â”‚   â”œâ”€â”€ 191.9hrs compute
â”‚   â””â”€â”€ Autosuspend after 5 min
â”œâ”€â”€ Upstash Free
â”‚   â”œâ”€â”€ 10K commands/day
â”‚   â”œâ”€â”€ 256MB storage
â”‚   â””â”€â”€ 100 connections
â”œâ”€â”€ Resend Free
â”‚   â”œâ”€â”€ 100 emails/day
â”‚   â””â”€â”€ 3,000 emails/month
â”œâ”€â”€ Sentry Developer (Free)
â”‚   â”œâ”€â”€ 5K errors/month
â”‚   â”œâ”€â”€ 10K transactions/month
â”‚   â”œâ”€â”€ 1 project
â”‚   â”œâ”€â”€ 30-day retention
â”‚   â””â”€â”€ Basic error tracking
â””â”€â”€ Worker Service - Render Free
    â”œâ”€â”€ 750hrs/month compute
    â”œâ”€â”€ 512MB RAM
    â””â”€â”€ Auto-sleep after 15 min inactivity

Suitable For:
âœ… Proof of concept
âœ… Customer demos
âœ… Internal testing
âœ… MVP validation
âœ… Learning/development

Not Suitable For:
âŒ Production with real users
âŒ High-frequency scraping
âŒ Real-time features
âŒ Mission-critical operations
```

#### **Setting Up Sentry (Free Tier)**

```bash
# 1. Install Sentry SDK
pnpm add @sentry/nextjs

# 2. Initialize Sentry
npx @sentry/wizard@latest -i nextjs

# 3. Configure Sentry (sentry.client.config.ts)
import * as Sentry from "@sentry/nextjs";

Sentry.init({
  dsn: process.env.NEXT_PUBLIC_SENTRY_DSN,
  
  // Free tier: 10K transactions/month
  // Sample 10% of transactions
  tracesSampleRate: 0.1,
  
  // Free tier: 5K errors/month
  // Report all errors
  sampleRate: 1.0,
  
  environment: process.env.NODE_ENV,
  
  // Don't capture errors in development
  enabled: process.env.NODE_ENV === 'production',
  
  // Performance monitoring
  integrations: [
    new Sentry.BrowserTracing({
      tracePropagationTargets: ["localhost", /^https:\/\/yourapp\.vercel\.app/],
    }),
  ],
});

# 4. Configure Sentry (sentry.server.config.ts)
import * as Sentry from "@sentry/nextjs";

Sentry.init({
  dsn: process.env.NEXT_PUBLIC_SENTRY_DSN,
  tracesSampleRate: 0.1,
  environment: process.env.NODE_ENV,
  enabled: process.env.NODE_ENV === 'production',
});

# 5. Add environment variables
NEXT_PUBLIC_SENTRY_DSN=https://xxx@xxx.ingest.sentry.io/xxx
SENTRY_AUTH_TOKEN=your-auth-token
SENTRY_ORG=your-org
SENTRY_PROJECT=your-project

# 6. Test Sentry integration
// pages/api/test-sentry.ts
import { NextApiRequest, NextApiResponse } from 'next';

export default function handler(req: NextApiRequest, res: NextApiResponse) {
  throw new Error('Sentry test error');
}
```

#### **Monitoring Best Practices (Free Tier)**

```typescript
// lib/monitoring/sentry.ts
import * as Sentry from '@sentry/nextjs';

// Capture errors with context
export function captureError(error: Error, context?: Record<string, any>) {
  Sentry.captureException(error, {
    extra: context,
    level: 'error',
  });
}

// Track scraping job failures
export function trackScrapingError(job: any, error: Error) {
  Sentry.captureException(error, {
    tags: {
      job_type: 'scraping',
      seller: job.seller,
      status: job.status,
    },
    extra: {
      jobId: job.id,
      url: job.url,
      attempts: job.attempts,
    },
  });
}

// Track performance issues
export function trackSlowQuery(query: string, duration: number) {
  if (duration > 1000) { // > 1 second
    Sentry.captureMessage(`Slow database query: ${query}`, {
      level: 'warning',
      tags: {
        type: 'performance',
        component: 'database',
      },
      extra: {
        duration,
        query,
      },
    });
  }
}

// Usage in API routes
export async function GET(request: Request) {
  try {
    const data = await fetchData();
    return Response.json(data);
  } catch (error) {
    captureError(error as Error, {
      route: '/api/products',
      method: 'GET',
    });
    return Response.json({ error: 'Internal server error' }, { status: 500 });
  }
}
```

#### **Free Tier Limits Management**

With Sentry free tier (5K errors/month, 10K transactions/month), implement smart error handling:

```typescript
// lib/monitoring/error-budget.ts
import { Redis } from 'ioredis';

const redis = new Redis(process.env.REDIS_HOST);

// Track error count for the month
async function shouldReportToSentry(errorType: string): Promise<boolean> {
  const key = `sentry:errors:${new Date().getMonth()}`;
  const count = await redis.incr(key);
  
  // Set expiry to end of month
  if (count === 1) {
    const now = new Date();
    const endOfMonth = new Date(now.getFullYear(), now.getMonth() + 1, 0);
    await redis.expireat(key, Math.floor(endOfMonth.getTime() / 1000));
  }
  
  // Free tier: 5K errors/month
  // Reserve 20% for critical errors
  const CRITICAL_THRESHOLD = 1000;
  const NORMAL_THRESHOLD = 4000;
  
  if (errorType === 'critical') {
    return count < CRITICAL_THRESHOLD;
  }
  
  return count < NORMAL_THRESHOLD;
}

// Smart error reporting
export async function reportError(error: Error, severity: 'critical' | 'normal') {
  const shouldReport = await shouldReportToSentry(severity);
  
  if (shouldReport) {
    Sentry.captureException(error);
  } else {
    // Log to console or alternative logging
    console.error('[Error budget exceeded]', error);
  }
}
```

---

### Phase 2: Starter Production (Launch)
**Cost**: $95-120/month  
**Timeline**: 3-6 months  
**Users**: 100-1,000 active users  
**Traffic**: 10K-100K pageviews/month

```
Infrastructure:
â”œâ”€â”€ Vercel Pro ($20/month)
â”‚   â”œâ”€â”€ Unlimited bandwidth
â”‚   â”œâ”€â”€ Unlimited function execution
â”‚   â”œâ”€â”€ 60s function timeout
â”‚   â”œâ”€â”€ Cron jobs enabled
â”‚   â”œâ”€â”€ Team collaboration
â”‚   â””â”€â”€ Advanced analytics
â”œâ”€â”€ Neon Launch ($19/month)
â”‚   â”œâ”€â”€ 10GB storage
â”‚   â”œâ”€â”€ Unlimited compute hours
â”‚   â”œâ”€â”€ 0.25 CU baseline
â”‚   â”œâ”€â”€ Connection pooling
â”‚   â””â”€â”€ 30-day point-in-time recovery
â”œâ”€â”€ Upstash Pay-as-you-go (~$10-20/month)
â”‚   â”œâ”€â”€ Unlimited commands
â”‚   â”œâ”€â”€ 1GB+ storage
â”‚   â”œâ”€â”€ 1,000 connections
â”‚   â””â”€â”€ Single region
â”œâ”€â”€ Resend Pro ($20/month)
â”‚   â”œâ”€â”€ 50,000 emails/month
â”‚   â”œâ”€â”€ Custom domains
â”‚   â”œâ”€â”€ Advanced analytics
â”‚   â””â”€â”€ Webhooks
â”œâ”€â”€ Worker Service - Render Starter ($7/month)
â”‚   â”œâ”€â”€ 512MB RAM
â”‚   â”œâ”€â”€ Always-on
â”‚   â”œâ”€â”€ Auto-deploy
â”‚   â””â”€â”€ Custom domain
â””â”€â”€ Sentry Team ($26/month)
    â”œâ”€â”€ 50K events/month
    â”œâ”€â”€ Error tracking
    â”œâ”€â”€ Performance monitoring
    â””â”€â”€ Release tracking

Total: ~$102/month

Scaling Triggers:
â†’ Phase 3 when:
  - >50GB database size
  - >500K pageviews/month
  - >1,000 concurrent users
  - Need for high availability
  - Multiple regions required
```

---

### Phase 3: Growth Production
**Cost**: $300-500/month  
**Timeline**: 6-12 months  
**Users**: 1,000-10,000 active users  
**Traffic**: 100K-1M pageviews/month

```
Infrastructure:
â”œâ”€â”€ Vercel Pro ($20/month)
â”‚   â””â”€â”€ (Same as Phase 2)
â”œâ”€â”€ Neon Scale ($69/month)
â”‚   â”œâ”€â”€ 50GB storage
â”‚   â”œâ”€â”€ 1 CU baseline (2x faster)
â”‚   â”œâ”€â”€ Autoscaling up to 4 CU
â”‚   â”œâ”€â”€ Read replicas (optional)
â”‚   â””â”€â”€ 30-day recovery
â”œâ”€â”€ Upstash Pro ($50/month)
â”‚   â”œâ”€â”€ Unlimited commands
â”‚   â”œâ”€â”€ 10GB storage
â”‚   â”œâ”€â”€ Multi-region replication
â”‚   â”œâ”€â”€ 10,000 connections
â”‚   â””â”€â”€ 99.99% SLA
â”œâ”€â”€ Resend Business ($80/month)
â”‚   â”œâ”€â”€ 1M emails/month
â”‚   â”œâ”€â”€ Dedicated IP
â”‚   â”œâ”€â”€ Priority support
â”‚   â””â”€â”€ Advanced deliverability
â”œâ”€â”€ Worker Service - Railway ($50/month)
â”‚   â”œâ”€â”€ 8GB RAM
â”‚   â”œâ”€â”€ 8 vCPU
â”‚   â”œâ”€â”€ Multiple workers
â”‚   â””â”€â”€ High availability
â””â”€â”€ Sentry Business ($99/month)
    â”œâ”€â”€ 500K events/month
    â”œâ”€â”€ Advanced features
    â”œâ”€â”€ Data forward
    â””â”€â”€ Custom retention

Total: ~$368/month

New Capabilities:
âœ… Multi-region deployment
âœ… Read replicas for scaling reads
âœ… Advanced monitoring
âœ… High availability setup
âœ… Dedicated support
```

---

### Phase 4: Enterprise Scale
**Cost**: $1,000+/month  
**Timeline**: 12+ months  
**Users**: 10,000+ active users  
**Traffic**: 1M+ pageviews/month

```
Infrastructure:
â”œâ”€â”€ Vercel Enterprise ($300+/month)
â”‚   â”œâ”€â”€ 300s function timeout
â”‚   â”œâ”€â”€ 3008MB function memory
â”‚   â”œâ”€â”€ 99.99% SLA
â”‚   â”œâ”€â”€ Dedicated support
â”‚   â”œâ”€â”€ Advanced security
â”‚   â””â”€â”€ Custom contracts
â”œâ”€â”€ Neon Scale+ (Custom pricing)
â”‚   â”œâ”€â”€ 200GB+ storage
â”‚   â”œâ”€â”€ Higher baseline compute
â”‚   â”œâ”€â”€ Multiple read replicas
â”‚   â”œâ”€â”€ 90-day recovery
â”‚   â””â”€â”€ Dedicated support
â”œâ”€â”€ Upstash Enterprise (Custom pricing)
â”‚   â”œâ”€â”€ Unlimited everything
â”‚   â”œâ”€â”€ Global replication
â”‚   â”œâ”€â”€ Custom SLA
â”‚   â””â”€â”€ Dedicated clusters
â”œâ”€â”€ Resend Enterprise (Custom pricing)
â”‚   â”œâ”€â”€ 10M+ emails/month
â”‚   â”œâ”€â”€ Multiple dedicated IPs
â”‚   â”œâ”€â”€ Custom SMTP
â”‚   â””â”€â”€ White-glove support
â”œâ”€â”€ Worker Service - AWS ECS/Kubernetes
â”‚   â”œâ”€â”€ Auto-scaling workers
â”‚   â”œâ”€â”€ Multi-region
â”‚   â”œâ”€â”€ Load balancing
â”‚   â””â”€â”€ Custom infrastructure
â””â”€â”€ Full Observability Stack
    â”œâ”€â”€ Sentry Enterprise
    â”œâ”€â”€ DataDog/New Relic
    â”œâ”€â”€ PagerDuty
    â””â”€â”€ Custom dashboards

Total: $1,000-5,000+/month

Enterprise Features:
âœ… Custom SLAs
âœ… Dedicated support teams
âœ… Custom contracts and pricing
âœ… Advanced security features
âœ… Compliance certifications
âœ… Multi-region failover
```

---

## Service-by-Service Scaling

### 1. Vercel: Hobby â†’ Pro Migration

#### **When to Upgrade**
- Need cron jobs for automated scraping
- Function timeout (10s) causing failures
- Team collaboration required
- Need advanced analytics

#### **Migration Steps**

```bash
# 1. In Vercel Dashboard
Settings â†’ General â†’ Plan â†’ Upgrade to Pro

# 2. Benefits Immediately Available:
âœ… Cron jobs enabled
âœ… 60s function timeout
âœ… Unlimited bandwidth
âœ… Unlimited function executions
âœ… Advanced analytics
âœ… Team members (no extra cost for first 10)

# 3. Update vercel.json to add cron jobs
{
  "crons": [
    {
      "path": "/api/cron/scraper",
      "schedule": "0 2 * * *"
    },
    {
      "path": "/api/cron/cleanup-jobs",
      "schedule": "*/30 * * * *"
    }
  ]
}

# 4. Deploy changes
git add vercel.json
git commit -m "Enable Vercel cron jobs"
git push origin main

# 5. Verify cron jobs
- Check Vercel Dashboard â†’ Cron Jobs
- Wait for scheduled time or trigger manually
```

#### **Post-Upgrade Optimizations**
```typescript
// Increase function timeout for heavy operations
export const config = {
  maxDuration: 60, // Now we can use up to 60s
};

// Enable ISR with longer revalidation
export const revalidate = 3600; // 1 hour

// Use edge functions for global performance
export const runtime = 'edge';
```

#### **Cost Impact**
- **Before**: $0/month
- **After**: $20/month per member
- **Break-even**: If bandwidth >100GB or need cron jobs

---

### 2. Neon: Free â†’ Launch/Scale Migration

#### **When to Upgrade**

**To Launch ($19/month)**:
- Database >0.5GB
- Need for always-on compute (no autosuspend)
- Require more than 191.9hrs/month compute
- Need 30-day point-in-time recovery

**To Scale ($69/month)**:
- Database >10GB
- Need faster queries (higher baseline compute)
- Require read replicas
- Need autoscaling under load

#### **Migration Steps**

```bash
# 1. Backup Current Data (Important!)
# In Neon Console:
Dashboard â†’ Branches â†’ Create branch from main
Name: backup-before-upgrade-2026-01-28

# Or export data
pg_dump $DATABASE_URL > backup.sql

# 2. Upgrade Plan
Dashboard â†’ Settings â†’ Plan â†’ Upgrade to Launch/Scale

# 3. Benefits Immediately Available:
âœ… No more autosuspend delays
âœ… Increased storage
âœ… Higher compute limits
âœ… Longer recovery window

# 4. Optimize Database for Paid Tier
```

```sql
-- Add indexes for better performance
CREATE INDEX idx_products_seller_id ON "Product"("sellerId");
CREATE INDEX idx_products_created_at ON "Product"("createdAt");
CREATE INDEX idx_scrapejobs_status ON "ScrapeJob"("status");
CREATE INDEX idx_scrapejobs_created_at ON "ScrapeJob"("createdAt");

-- Analyze tables for query optimization
ANALYZE "Product";
ANALYZE "ScrapeJob";
ANALYZE "User";

-- Set up connection pooling (already included, but verify)
-- Neon automatically uses PgBouncer for pooling
```

#### **Configure Autoscaling (Scale tier only)**

```bash
# In Neon Console â†’ Settings â†’ Compute:
1. Baseline: 0.25 CU â†’ 1 CU (4x faster)
2. Autoscaling maximum: 4 CU (16x burst capacity)
3. Autosuspend delay: Disable (always-on)
```

#### **Set Up Read Replicas (Scale tier)**

```bash
# 1. Create read replica
Dashboard â†’ Branches â†’ Create replica
Type: Read replica
Region: Same as primary (or different for geo-distribution)

# 2. Get replica connection string
REPLICA_DATABASE_URL="postgresql://user:pass@ep-xxx-replica.aws.neon.tech/db"

# 3. Update application to use replica for reads
```

```typescript
// lib/prisma.ts
import { PrismaClient } from '@prisma/client';

// Primary database (writes)
export const prisma = new PrismaClient({
  datasources: {
    db: {
      url: process.env.DATABASE_URL,
    },
  },
});

// Read replica (reads only)
export const prismaRead = new PrismaClient({
  datasources: {
    db: {
      url: process.env.REPLICA_DATABASE_URL || process.env.DATABASE_URL,
    },
  },
});

// Usage example
export async function getProducts() {
  // Use read replica for better performance
  return await prismaRead.product.findMany();
}

export async function createProduct(data) {
  // Use primary for writes
  return await prisma.product.create({ data });
}
```

#### **Cost Impact**
- **Free â†’ Launch**: $0 â†’ $19/month (+10GB, no limits)
- **Launch â†’ Scale**: $19 â†’ $69/month (+40GB, faster, replicas)
- **Break-even**: If database >0.5GB or need always-on

---

### 3. Upstash: Free â†’ Paid Migration

#### **When to Upgrade**

**To Pay-as-you-go (~$10-20/month)**:
- Exceeding 10K commands/day
- Need >256MB storage
- Queue jobs piling up
- Cache eviction causing issues

**To Pro ($50/month)**:
- Need multi-region replication
- Require 99.99% SLA
- Need >10GB storage
- High availability required

#### **Migration Steps**

```bash
# 1. Analyze Current Usage
# In Upstash Console â†’ Metrics:
- Daily commands count
- Storage usage
- Peak connection count
- Command types breakdown

# 2. Estimate Cost
# Pay-as-you-go pricing:
$0.2 per 100K commands
$0.25 per GB storage

# Example calculation:
# 100K commands/day = 3M/month
# Cost: (3,000,000 / 100,000) * $0.2 = $6/month

# 3. Upgrade Plan
Dashboard â†’ Settings â†’ Plan â†’ Upgrade

# 4. No downtime - connection details remain same
# Just different billing
```

#### **Optimize Redis Usage After Upgrade**

```typescript
// lib/cache/redis-cache.ts
import { Redis } from 'ioredis';

const redis = new Redis({
  host: process.env.REDIS_HOST,
  port: parseInt(process.env.REDIS_PORT!),
  password: process.env.REDIS_PASSWORD,
  // Now we can use more aggressive caching
  maxRetriesPerRequest: 5, // Increased
  connectTimeout: 30000,   // Longer timeout
  retryStrategy: (times) => Math.min(times * 100, 3000),
});

// Implement cache warming for paid tier
export async function warmCache() {
  // Pre-populate cache with hot data
  const sellers = await prisma.seller.findMany();
  for (const seller of sellers) {
    await redis.setex(
      `seller:${seller.id}`,
      3600, // 1 hour TTL
      JSON.stringify(seller)
    );
  }
}

// Use Redis for more features
export async function getRateLimitWithoutWorry(userId: string) {
  // With paid tier, we don't worry about command limits
  const key = `ratelimit:${userId}`;
  const count = await redis.incr(key);
  await redis.expire(key, 60); // 1 minute window
  return count;
}
```

#### **Set Up Multi-Region Replication (Pro tier)**

```bash
# 1. Enable global replication
Dashboard â†’ Settings â†’ Replication
Regions: us-east-1, eu-west-1, ap-southeast-1

# 2. Update application to use closest region
```

```typescript
// Auto-detect closest region
const REDIS_REGIONS = {
  'us-east-1': process.env.REDIS_HOST_US,
  'eu-west-1': process.env.REDIS_HOST_EU,
  'ap-southeast-1': process.env.REDIS_HOST_AP,
};

function getClosestRedis(userRegion: string) {
  return new Redis({
    host: REDIS_REGIONS[userRegion] || REDIS_REGIONS['us-east-1'],
    // ... other config
  });
}
```

#### **Cost Impact**
- **Free â†’ Pay-as-you-go**: $0 â†’ ~$10-20/month (usage-based)
- **Pay-as-you-go â†’ Pro**: Variable â†’ $50/month (unlimited)
- **Break-even**: If >10K commands/day or need HA

---

### 4. Resend: Free â†’ Pro/Business Migration

#### **When to Upgrade**

**To Pro ($20/month)**:
- Sending >100 emails/day
- Need >3,000 emails/month
- Want custom domain
- Need webhooks and analytics

**To Business ($80/month)**:
- Need dedicated IP
- Sending >50K emails/month
- Priority deliverability
- Advanced analytics required

#### **Migration Steps**

```bash
# 1. Upgrade Plan
Dashboard â†’ Billing â†’ Upgrade to Pro/Business

# 2. Benefits Immediately Available:
âœ… Higher email limits
âœ… Custom domain support
âœ… Webhooks enabled
âœ… Advanced analytics
âœ… Priority support (Business)
âœ… Dedicated IP (Business)

# 3. Configure Dedicated IP (Business tier only)
Dashboard â†’ Dedicated IPs â†’ Request IP
- IP assigned within 24-48 hours
- Warm up IP gradually (provided schedule)
```

#### **Set Up Webhooks (Pro/Business)**

```typescript
// app/api/webhooks/resend/route.ts
import { headers } from 'next/headers';
import { NextResponse } from 'next/server';

export async function POST(request: Request) {
  // Verify webhook signature
  const signature = headers().get('resend-signature');
  
  const event = await request.json();
  
  switch (event.type) {
    case 'email.sent':
      await handleEmailSent(event.data);
      break;
    case 'email.delivered':
      await handleEmailDelivered(event.data);
      break;
    case 'email.bounced':
      await handleEmailBounced(event.data);
      break;
    case 'email.complained':
      await handleEmailComplained(event.data);
      break;
  }
  
  return NextResponse.json({ received: true });
}

async function handleEmailBounced(data: any) {
  // Mark user email as bounced in database
  await prisma.user.update({
    where: { email: data.email },
    data: { emailBounced: true, emailBouncedAt: new Date() }
  });
  
  // Log for investigation
  apiLogger.logError('Email bounced', { data });
}
```

#### **Advanced Email Features**

```typescript
// lib/services/email-service.ts
import { Resend } from 'resend';

const resend = new Resend(process.env.RESEND_API_KEY);

// Batch sending (Pro/Business)
export async function sendBatchEmails(emails: Email[]) {
  return await resend.batch.send(
    emails.map(email => ({
      from: 'GoodSeed <noreply@lembooking.com>',
      to: email.to,
      subject: email.subject,
      react: email.template,
      tags: [
        { name: 'category', value: email.category },
        { name: 'user_id', value: email.userId },
      ],
    }))
  );
}

// Track email analytics
export async function getEmailAnalytics(emailId: string) {
  return await resend.emails.get(emailId);
}

// With dedicated IP (Business), configure custom domain
export async function sendFromCustomDomain(email: Email) {
  return await resend.emails.send({
    from: 'support@goodseed.com', // Your verified domain
    to: email.to,
    subject: email.subject,
    react: email.template,
  });
}
```

#### **Cost Impact**
- **Free â†’ Pro**: $0 â†’ $20/month (+47K emails)
- **Pro â†’ Business**: $20 â†’ $80/month (dedicated IP, 1M emails)
- **Break-even**: If sending >100 emails/day

---

### 5. Background Workers: Vercel â†’ Dedicated Service

#### **When to Migrate**

Current Setup Issues:
- Scraping jobs timing out (>60s)
- Function execution costs high
- Need always-on workers
- Require more memory/CPU
- Complex scraping with Puppeteer

#### **Migration Options**

**Option A: Render ($7-50/month)**
```bash
Best for: Small to medium workloads
Pros:
- Simple setup
- Auto-deploy from GitHub
- Free tier available (with sleep)
- Good for single worker

Cons:
- Free tier sleeps (15 min inactivity)
- Limited scaling options
- Single region only (free/starter)
```

**Option B: Railway ($5-100/month)**
```bash
Best for: Growing teams
Pros:
- Pay-per-use pricing
- Great developer experience
- Multi-service support
- Good scaling

Cons:
- Can get expensive at scale
- Limited regions
```

**Option C: Fly.io (Pay-as-you-go)**
```bash
Best for: Global distribution
Pros:
- Global deployment
- Great for latency-sensitive apps
- Good pricing
- Excellent CLI

Cons:
- More complex setup
- Requires Dockerfile knowledge
```

**Option D: AWS ECS/Fargate (Enterprise)**
```bash
Best for: Enterprise, full control
Pros:
- Unlimited scaling
- Full control
- Integration with AWS services
- Enterprise support

Cons:
- Complex setup
- Higher costs
- Requires DevOps expertise
```

#### **Migration Steps (Render Example)**

```bash
# 1. Create Dockerfile.worker (if not exists)
# See BACKGROUND-WORKERS.md for complete Dockerfile

# 2. Create Render Account
https://render.com â†’ Sign up with GitHub

# 3. Create Background Worker
New â†’ Background Worker
Repository: goodseed-app-vercel
Name: goodseed-worker
Environment: Docker
Dockerfile: ./Dockerfile.worker
Plan:
  - Free: $0 (with sleep)
  - Starter: $7/month (always-on, 512MB)
  - Standard: $25/month (2GB RAM)
  - Pro: $85/month (8GB RAM)

# 4. Add Environment Variables
# Copy all from Vercel settings

# 5. Deploy
# Render auto-builds and deploys

# 6. Update WORKER_HEALTH_URL in Vercel
WORKER_HEALTH_URL=https://goodseed-worker.onrender.com/health

# 7. Test Worker
curl https://goodseed-worker.onrender.com/health
```

#### **Worker Scaling Configuration**

```yaml
# render.yaml (for infrastructure as code)
services:
  - type: worker
    name: goodseed-scraper-worker
    env: docker
    dockerfilePath: ./Dockerfile.worker
    plan: standard # 2GB RAM
    envVars:
      - key: DATABASE_URL
        sync: false
      - key: REDIS_HOST
        sync: false
      # ... other vars
    scaling:
      minInstances: 1
      maxInstances: 5 # Auto-scale based on load
      targetCPUPercent: 70
      targetMemoryPercent: 80

  - type: worker
    name: goodseed-email-worker
    env: docker
    dockerfilePath: ./Dockerfile.email-worker
    plan: starter # 512MB (email is lighter)
    scaling:
      minInstances: 1
      maxInstances: 2
```

#### **Cost Impact**
- **Vercel Functions**: Variable, expensive at scale
- **Render Starter**: $7/month (always-on)
- **Render Standard**: $25/month (better performance)
- **Railway**: ~$10-50/month (usage-based)

---

## Migration Procedures

### Zero-Downtime Migration Strategy

#### **1. Pre-Migration Preparation (1 week before)**

```bash
# Checklist:
â–¡ Document current performance metrics
â–¡ Set up monitoring (if not already)
â–¡ Create backups of all data
â–¡ Test migration in staging environment
â–¡ Notify team and stakeholders
â–¡ Schedule migration during low-traffic period
â–¡ Prepare rollback plan
```

#### **2. Database Migration (Neon)**

```bash
# Option A: In-place upgrade (recommended)
# - No downtime
# - Connection strings stay same
# - Instant upgrade

1. Neon Console â†’ Settings â†’ Plan â†’ Upgrade
2. Confirm upgrade
3. Wait for confirmation (< 1 minute)
4. Verify in dashboard

# Option B: Create new database and migrate
# - More control
# - Test before switching
# - Requires downtime or dual-write period

1. Create new Neon project with paid tier
2. Export data from old database:
   pg_dump $OLD_DATABASE_URL > dump.sql

3. Import to new database:
   psql $NEW_DATABASE_URL < dump.sql

4. Update application environment variables:
   DATABASE_URL=$NEW_DATABASE_URL

5. Deploy application
6. Verify everything works
7. Delete old database
```

#### **3. Redis Migration (Upstash)**

```bash
# Option A: In-place upgrade (recommended)
# - No downtime
# - Connection stays same

1. Upstash Console â†’ Settings â†’ Plan â†’ Upgrade
2. Select new plan
3. Confirm upgrade
4. Connection details unchanged

# Option B: Create new Redis and migrate
# - For changing regions or major restructuring

1. Create new Upstash Redis with desired plan
2. Implement dual-write temporarily:

```typescript
// Temporary dual-write during migration
const oldRedis = new Redis(process.env.OLD_REDIS_HOST);
const newRedis = new Redis(process.env.NEW_REDIS_HOST);

async function setCache(key: string, value: string, ttl: number) {
  await Promise.all([
    oldRedis.setex(key, ttl, value),
    newRedis.setex(key, ttl, value),
  ]);
}
```

```bash
3. Monitor new Redis performance
4. Switch reads to new Redis:
   REDIS_HOST=$NEW_REDIS_HOST

5. Deploy application
6. Stop dual-write after 24 hours
7. Delete old Redis
```

#### **4. Email Service Migration (Resend)**

```bash
# Simple upgrade (no migration needed)
1. Resend Dashboard â†’ Billing â†’ Upgrade
2. Select plan
3. Confirm
4. Features immediately available

# No code changes required
# Same API keys work
# No downtime
```

#### **5. Application Migration (Vercel)**

```bash
# Simple upgrade (no migration needed)
1. Vercel Dashboard â†’ Settings â†’ Plan â†’ Upgrade
2. Select Pro or Enterprise
3. Confirm payment
4. Features immediately available

# Add cron jobs (now available)
5. Create vercel.json with cron configuration
6. Commit and push
7. Verify cron jobs in dashboard
```

---

## Performance Benchmarks

### Before (Free Tier) vs After (Paid Tier)

#### **Page Load Performance**

| Metric | Free Tier | Pro Tier | Improvement |
|--------|-----------|----------|-------------|
| TTFB (Time to First Byte) | 800ms | 200ms | **4x faster** |
| FCP (First Contentful Paint) | 1.8s | 0.6s | **3x faster** |
| LCP (Largest Contentful Paint) | 3.2s | 1.1s | **3x faster** |
| Database Query Time (p95) | 450ms | 80ms | **5.6x faster** |
| API Response Time | 650ms | 150ms | **4.3x faster** |

#### **Scraping Performance**

| Metric | Free (Vercel Functions) | Paid (Dedicated Worker) | Improvement |
|--------|-------------------------|-------------------------|-------------|
| Max Job Duration | 10s | Unlimited | **No limit** |
| Concurrent Jobs | 1-2 | 10-50 | **25x more** |
| Success Rate | 60% (timeouts) | 98% | **38% better** |
| Products/Hour | ~500 | ~5,000 | **10x more** |
| Cost per 1K Products | High (function costs) | Low (flat rate) | **5-10x cheaper** |

#### **Email Delivery**

| Metric | Free Tier | Pro Tier | Improvement |
|--------|-----------|----------|-------------|
| Daily Limit | 100 emails | 1,666 emails | **16x more** |
| Deliverability Rate | 92% | 98% | **6% better** |
| Bounce Rate | 5% | 1% | **5x better** |
| Delivery Time (p95) | 15s | 3s | **5x faster** |

---

## Cost Optimization

### Strategies to Minimize Costs While Scaling

#### **1. Right-Size Your Infrastructure**

```bash
# Don't over-provision
Start with:
- Vercel Pro (not Enterprise)
- Neon Launch (not Scale)  
- Upstash Pay-as-you-go (not Pro)
- Resend Pro (not Business)

# Scale up only when metrics show need
Monitor monthly and adjust
```

#### **2. Optimize Database Usage**

```sql
-- Add indexes to reduce query time
CREATE INDEX CONCURRENTLY idx_hot_queries 
ON "Product"("sellerId", "createdAt");

-- Use partial indexes for specific queries
CREATE INDEX idx_active_products 
ON "Product"("status") 
WHERE "status" = 'ACTIVE';

-- Regular maintenance
VACUUM ANALYZE;
```

```typescript
// Use connection pooling effectively
const prisma = new PrismaClient({
  datasources: {
    db: {
      url: process.env.DATABASE_URL,
    },
  },
  log: process.env.NODE_ENV === 'development' ? ['query'] : [],
});

// Batch queries to reduce round trips
const [products, sellers] = await Promise.all([
  prisma.product.findMany(),
  prisma.seller.findMany(),
]);

// Use select to fetch only needed fields
const products = await prisma.product.findMany({
  select: {
    id: true,
    name: true,
    price: true,
    // Don't fetch large text fields unnecessarily
  },
});
```

#### **3. Optimize Redis Usage**

```typescript
// Set appropriate TTLs
await redis.setex('product:list', 300, data); // 5 min for lists
await redis.setex('product:detail', 3600, data); // 1 hour for details

// Use pipeline for multiple operations
const pipeline = redis.pipeline();
pipeline.get('key1');
pipeline.get('key2');
pipeline.get('key3');
const results = await pipeline.exec();

// Clean up old keys regularly
await redis.del('old:keys:*');
```

#### **4. Optimize Email Usage**

```typescript
// Batch emails when possible
await resend.batch.send(emails);

// Use templates to reduce payload size
// Store templates in Resend, not in API calls

// Implement user preferences
// Don't send emails if user opted out
if (user.emailNotifications) {
  await sendEmail(user.email, template);
}

// Consolidate notifications
// Send daily digest instead of individual emails
```

#### **5. Optimize Worker Usage**

```typescript
// Process jobs in batches
const jobs = await scraperQueue.getWaiting(10);
await Promise.all(jobs.map(job => processJob(job)));

// Use concurrency limits
scraperQueue.process(5, async (job) => {
  // Process up to 5 jobs concurrently
});

// Clean up completed jobs
await scraperQueue.clean(7 * 24 * 60 * 60 * 1000); // 7 days
```

#### **6. Use Caching Aggressively**

```typescript
// Edge caching for static content
export const revalidate = 3600; // 1 hour

// Redis caching for API responses
async function getCachedProducts() {
  const cached = await redis.get('products:all');
  if (cached) return JSON.parse(cached);
  
  const products = await prisma.product.findMany();
  await redis.setex('products:all', 300, JSON.stringify(products));
  return products;
}

// Browser caching
export async function GET(request: Request) {
  return new Response(data, {
    headers: {
      'Cache-Control': 'public, max-age=3600, s-maxage=3600',
    },
  });
}
```

### Monthly Cost Tracking

```typescript
// Track costs in monitoring system
interface MonthlyCosts {
  vercel: {
    plan: number;
    bandwidth: number;
    functionExecutions: number;
  };
  neon: {
    plan: number;
    storage: number;
    compute: number;
  };
  upstash: {
    plan: number;
    commands: number;
    storage: number;
  };
  resend: {
    plan: number;
    emails: number;
  };
  workers: {
    plan: number;
    instances: number;
  };
  monitoring: {
    sentry: number;
    other: number;
  };
  total: number;
}

// Set up alerts for cost anomalies
async function checkCostAnomalies(costs: MonthlyCosts) {
  if (costs.total > BUDGET_THRESHOLD) {
    await sendAlert({
      type: 'cost-anomaly',
      message: `Monthly costs ($${costs.total}) exceed budget ($${BUDGET_THRESHOLD})`,
      breakdown: costs,
    });
  }
}
```

---

## Scaling Checklist

### Pre-Scaling (1 week before)

- [ ] Review current usage metrics
- [ ] Identify bottlenecks
- [ ] Calculate expected costs
- [ ] Get budget approval
- [ ] Create backups
- [ ] Set up staging environment
- [ ] Test migration procedure
- [ ] Document rollback plan
- [ ] Notify team
- [ ] Schedule migration window

### During Scaling

- [ ] Start with lowest-priority service
- [ ] Upgrade one service at a time
- [ ] Verify each upgrade before continuing
- [ ] Monitor performance metrics
- [ ] Check error rates
- [ ] Test critical user flows
- [ ] Update documentation
- [ ] Communicate status to team

### Post-Scaling (1 week after)

- [ ] Verify all services stable
- [ ] Check cost actuals vs estimates
- [ ] Review performance improvements
- [ ] Optimize based on new metrics
- [ ] Update monitoring alerts
- [ ] Document lessons learned
- [ ] Train team on new features
- [ ] Plan next scaling phase

---

## Rollback Procedures

### If Scaling Causes Issues

#### **Immediate Rollback (< 1 hour)**

```bash
# For Vercel
1. Dashboard â†’ Deployments
2. Find last working deployment
3. Click "Promote to Production"

# For Neon
1. Create branch from backup
2. Update DATABASE_URL
3. Redeploy application

# For Upstash
1. Switch back to old Redis
2. Update REDIS_HOST
3. Redeploy application

# For Resend
1. No rollback needed (can downgrade anytime)
2. Features remain available until next billing cycle
```

#### **Gradual Rollback (1-24 hours)**

```bash
# If immediate rollback not needed:
1. Identify specific issue
2. Fix issue in paid tier environment
3. Keep paid tier for benefits
4. Adjust configuration rather than rollback

# Example: Database too slow
- Don't rollback
- Instead: optimize queries, add indexes
- Scale up compute if needed
```

---

## Common Scaling Mistakes to Avoid

### âŒ Mistake 1: Scaling Everything at Once
**Problem**: Can't identify which change caused issues  
**Solution**: Scale one service at a time, verify each

### âŒ Mistake 2: Not Testing in Staging First
**Problem**: Issues discovered in production  
**Solution**: Always test scaling in staging environment

### âŒ Mistake 3: Ignoring Database Indexes
**Problem**: Slow queries despite paid tier  
**Solution**: Add indexes before scaling, optimize queries

### âŒ Mistake 4: Over-Provisioning
**Problem**: Paying for unused capacity  
**Solution**: Start small, scale based on actual metrics

### âŒ Mistake 5: Not Monitoring After Scaling
**Problem**: Don't know if scaling helped  
**Solution**: Compare metrics before and after, adjust accordingly

---

## Summary

Scaling from free tier to production is a gradual process:

1. **Start Free**: Validate your MVP
2. **Scale When Needed**: Based on actual limits hit
3. **One Service at a Time**: Easier to troubleshoot
4. **Monitor Everything**: Make data-driven decisions
5. **Optimize First**: Before throwing money at problems
6. **Plan Ahead**: Know your scaling triggers

**Key Metrics to Watch**:
- Database size and query performance
- Redis command usage
- Email sending volume
- Worker job processing time
- User growth rate
- Cost per user

**Next Steps**:
- See [MONITORING.md](./MONITORING.md) for tracking metrics
- See [COST-ANALYSIS.md](./COST-ANALYSIS.md) for detailed cost breakdown
- See [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) for common issues

---

**Remember**: Scale based on data, not assumptions. Monitor first, then scale smart! ðŸ“ˆ
