/**
 * SimplePoliteCrawler - C√¥ng c·ª• Crawl Web C√≥ ƒê·∫°o ƒê·ª©c
 * 
 * Class n√†y implement c√°c th·ª±c h√†nh crawling l·ªãch s·ª± bao g·ªìm:
 * - Tu√¢n th·ªß robots.txt v·ªõi custom parsing
 * - Delay ƒë·ªông gi·ªØa c√°c requests
 * - X·ª≠ l√Ω HTTP status codes
 * - ƒê·ªãnh danh User-Agent ph√π h·ª£p
 * 
 * T√≠nh nƒÉng ch√≠nh:
 * - Custom robots.txt parser (parser built-in c·ªßa Crawlee c√≥ l·ªói)
 * - Cache robots.txt files ƒë·ªÉ gi·∫£m requests
 * - H·ªó tr·ª£ Allow/Disallow rules v·ªõi th·ª© t·ª± ∆∞u ti√™n ƒë√∫ng
 * - Wildcard pattern matching cho robots.txt rules ph·ª©c t·∫°p
 * - T·ª± ƒë·ªông backoff khi b·ªã rate limiting v√† server errors
 */

import { RobotsTxtFile } from "crawlee";
import { apiLogger } from "../helpers/api-logger";
import { MAX_DELAY_DEFAULT, MIN_DELAY_DEFAULT } from "@/scrapers/(common)/constants";

/**
 * C√°c t√πy ch·ªçn c·∫•u h√¨nh cho SimplePoliteCrawler
 */
interface PoliteCrawlerOptions {
    userAgent: string;         // User-Agent string ƒë·ªÉ ƒë·ªãnh danh crawler
    acceptLanguage?: string;   // Accept-Language header cho c√°c site qu·ªëc t·∫ø
    minDelay?: number;         // Delay t·ªëi thi·ªÉu gi·ªØa c√°c requests (ms)
    maxDelay?: number;         // Delay t·ªëi ƒëa gi·ªØa c√°c requests (ms)
}

/**
 * C·∫•u tr√∫c cache ƒë·ªÉ l∆∞u tr·ªØ robots.txt files v·ªõi timestamps
 * NgƒÉn ch·∫∑n vi·ªác fetch l·∫∑p l·∫°i robots.txt cho c√πng m·ªôt domain
 */
interface RobotsCache {
    [origin: string]: { robotsTxt: RobotsTxtFile; timestamp: number; };
}

/**
 * Rules c·ªßa site origin t·ª´ robots.txt
 */
export interface RobotsRules {
    crawlDelay: number;
        disallowedPaths: string[];
        allowedPaths: string[];
        userAgent: string;
} 

export class SimplePoliteCrawler {
    // Cache ƒë·ªÉ l∆∞u robots.txt theo domain, tr√°nh fetch l·∫∑p l·∫°i
    private robotsCache: RobotsCache = {};
    // Th·ªùi gian cache robots.txt (24 gi·ªù)
    private readonly cacheExpiry = 24 * 60 * 60 * 1000;
    // User-Agent string ƒë·ªÉ ƒë·ªãnh danh crawler
    private readonly userAgent: string;
    // Accept-Language header cho requests
    private readonly acceptLanguage: string;
    // Delay t·ªëi thi·ªÉu v√† t·ªëi ƒëa gi·ªØa c√°c requests (ms)
    private readonly minDelay: number;
    private readonly maxDelay: number;

    /**
     * Constructor - Kh·ªüi t·∫°o SimplePoliteCrawler v·ªõi c√°c t√πy ch·ªçn
     */
    constructor(options: PoliteCrawlerOptions) {
        this.userAgent = options.userAgent;
        this.acceptLanguage = options.acceptLanguage || 'en-US,en;q=0.9';
        this.minDelay = options.minDelay || MIN_DELAY_DEFAULT;
        this.maxDelay = options.maxDelay || MAX_DELAY_DEFAULT;
    }

    /**
     * L·∫•y robots.txt file t·ª´ origin, c√≥ cache ƒë·ªÉ t·ªëi ∆∞u performance
     * @param origin - Domain g·ªëc (vd: https://example.com)
     * @returns RobotsTxtFile object ho·∫∑c null n·∫øu kh√¥ng t√¨m th·∫•y
     */
    async getRobotsTxt(origin: string): Promise<RobotsTxtFile | null> {
        const now = Date.now();
        const cached = this.robotsCache[origin];

        // Ki·ªÉm tra cache tr∆∞·ªõc khi fetch m·ªõi
        if (cached && (now - cached.timestamp) < this.cacheExpiry) {
            return cached.robotsTxt;
        }

        try {
            const robotsTxtUrl = `${origin}/robots.txt`;
            const robotsTxt = await RobotsTxtFile.find(robotsTxtUrl);

            // L∆∞u v√†o cache v·ªõi timestamp
            this.robotsCache[origin] = { robotsTxt, timestamp: now };

            return robotsTxt;
        } catch (error) {
            apiLogger.warn(`Kh√¥ng th·ªÉ fetch robots.txt cho ${origin}:`, { error });
            return null;
        }
    }

    /**
     * Parse robots.txt v√† tr·∫£ v·ªÅ crawling rules object
     * @param baseUrl - Base URL c·ªßa site (e.g., https://www.canukseeds.com)
     * @returns Robots rules object v·ªõi crawlDelay, disallowedPaths, allowedPaths
     */
    async parseRobots(baseUrl: string): Promise<RobotsRules> {
        try {
            const origin = new URL(baseUrl).origin;
            
            // Fetch robots.txt content tr·ª±c ti·∫øp
            const robotsResponse = await fetch(`${origin}/robots.txt`);
            if (!robotsResponse.ok) {
                apiLogger.warn(`Kh√¥ng t√¨m th·∫•y robots.txt cho ${origin}, s·ª≠ d·ª•ng default settings`);
                return {
                    crawlDelay: this.getRandomDelay(),
                    disallowedPaths: [],
                    allowedPaths: ['*'],
                    userAgent: this.userAgent
                };
            }
            
            const robotsContent = await robotsResponse.text();
            const lines = robotsContent.split('\n').map(line => line.trim()).filter(line => line && !line.startsWith('#'));
            
            // Parse robots.txt rules
            let currentUserAgent = '';
            let applicableUserAgent = false;
            const disallowedPaths: string[] = [];
            const allowedPaths: string[] = [];
            let crawlDelaySeconds = 0;
            
            for (const line of lines) {
                const lowerLine = line.toLowerCase();
                
                // X·ª≠ l√Ω User-agent directive
                if (lowerLine.startsWith('user-agent:')) {
                    const ua = line.substring(11).trim();
                    currentUserAgent = ua;
                    // Ki·ªÉm tra xem user-agent section n√†y c√≥ √°p d·ª•ng cho ch√∫ng ta kh√¥ng
                    applicableUserAgent = ua === '*' || 
                        this.userAgent.toLowerCase().includes(ua.toLowerCase()) ||
                        ua.toLowerCase().includes(this.userAgent.split('/')[0].toLowerCase());
                    continue;
                }
                
                // Ch·ªâ parse rules n·∫øu thu·ªôc user-agent section √°p d·ª•ng cho ch√∫ng ta
                if (applicableUserAgent) {
                    if (lowerLine.startsWith('disallow:')) {
                        const pattern = line.substring(9).trim();
                        if (pattern) {
                            disallowedPaths.push(pattern);
                        }
                    } else if (lowerLine.startsWith('allow:')) {
                        const pattern = line.substring(6).trim();
                        if (pattern) {
                            allowedPaths.push(pattern);
                        }
                    } else if (lowerLine.startsWith('crawl-delay:')) {
                        const delay = parseFloat(line.substring(12).trim());
                        if (!isNaN(delay)) {
                            crawlDelaySeconds = delay;
                        }
                    }
                }
            }
            
            const crawlDelayMs = crawlDelaySeconds > 0 ? crawlDelaySeconds * 1000 : this.getRandomDelay();
            
            apiLogger.info(`üìã Robots.txt parsed cho ${origin}:`);
            apiLogger.info(`   ‚è±Ô∏è Crawl delay: ${crawlDelayMs}ms`);
            apiLogger.info(`   ‚ùå Disallowed paths: ${disallowedPaths.length}`);
            apiLogger.info(`   ‚úÖ Allowed paths: ${allowedPaths.length}`);
            
            return {
                crawlDelay: crawlDelayMs,
                disallowedPaths,
                allowedPaths,
                userAgent: this.userAgent
            };
            
        } catch (error) {
            apiLogger.warn(`L·ªói khi parse robots.txt cho ${baseUrl}:`, { error });
            return {
                crawlDelay: this.getRandomDelay(),
                disallowedPaths: [],
                allowedPaths: ['*'],
                userAgent: this.userAgent
            };
        }
    }

    /**
     * Apply delay theo robots.txt crawl-delay ho·∫∑c custom delay
     * @param delayMs - Delay time in milliseconds
     */
    async delay(delayMs?: number): Promise<void> {
        const actualDelay = delayMs || this.getRandomDelay();
        apiLogger.debug(`‚è±Ô∏è Applying crawl delay: ${actualDelay}ms`);
        
        return new Promise(resolve => {
            setTimeout(resolve, actualDelay);
        });
    }

    /**
     * Ki·ªÉm tra xem URL c√≥ ƒë∆∞·ª£c ph√©p crawl theo robots.txt kh√¥ng
     * S·ª≠ d·ª•ng custom parser thay v√¨ Crawlee's buggy implementation
     * @param url - URL c·∫ßn ki·ªÉm tra
     * @returns true n·∫øu ƒë∆∞·ª£c ph√©p, false n·∫øu b·ªã c·∫•m
     */
    async isAllowed(url: string): Promise<boolean> {
        try {
            const origin = new URL(url).origin;
            
            // S·ª≠ d·ª•ng custom parser thay v√¨ Crawlee's buggy implementation
            const isAllowedByRobots = await this.checkRobotsPermission(url);
            
            apiLogger.info(`Ki·ªÉm tra robots.txt cho ${url}: ${isAllowedByRobots ? 'ƒê∆Ø·ª¢C PH√âP' : 'B·ªä CH·∫∂N'}`);
            return isAllowedByRobots;
        } catch (error) {
            apiLogger.warn(`L·ªói khi ki·ªÉm tra robots.txt cho ${url}:`, { error });
            return true; // M·∫∑c ƒë·ªãnh cho ph√©p khi c√≥ l·ªói
        }
    }

    /**
     * Custom robots.txt parser ƒë·ªÉ x·ª≠ l√Ω Allow/Disallow rules m·ªôt c√°ch ch√≠nh x√°c
     * Crawlee's RobotsTxtFile.isAllowed() c√≥ bugs v√† kh√¥ng respect disallow rules
     * @param url - URL c·∫ßn ki·ªÉm tra permission
     * @returns true n·∫øu ƒë∆∞·ª£c ph√©p crawl, false n·∫øu b·ªã c·∫•m
     */
    private async checkRobotsPermission(url: string): Promise<boolean> {
        try {
            const origin = new URL(url).origin;
            const urlPath = new URL(url).pathname;
            const fullUrl = url;
            
            // Fetch robots.txt content tr·ª±c ti·∫øp
            const robotsResponse = await fetch(`${origin}/robots.txt`);
            if (!robotsResponse.ok) {
                apiLogger.warn(`Kh√¥ng t√¨m th·∫•y robots.txt cho ${origin}, m·∫∑c ƒë·ªãnh cho ph√©p`);
                return true;
            }
            
            const robotsContent = await robotsResponse.text();
            // Parse content, lo·∫°i b·ªè comments v√† empty lines
            const lines = robotsContent.split('\n').map(line => line.trim()).filter(line => line && !line.startsWith('#'));
            
            // Parse robots.txt rules
            let currentUserAgent = '';
            let applicableUserAgent = false;
            const rules: { type: 'allow' | 'disallow'; pattern: string }[] = [];
            
            for (const line of lines) {
                const lowerLine = line.toLowerCase();
                
                // X·ª≠ l√Ω User-agent directive
                if (lowerLine.startsWith('user-agent:')) {
                    const ua = line.substring(11).trim();
                    currentUserAgent = ua;
                    // Ki·ªÉm tra xem user-agent section n√†y c√≥ √°p d·ª•ng cho ch√∫ng ta kh√¥ng
                    applicableUserAgent = ua === '*' || 
                        this.userAgent.toLowerCase().includes(ua.toLowerCase()) ||
                        ua.toLowerCase().includes(this.userAgent.split('/')[0].toLowerCase());
                    continue;
                }
                
                // Ch·ªâ parse rules n·∫øu thu·ªôc user-agent section √°p d·ª•ng cho ch√∫ng ta
                if (applicableUserAgent) {
                    if (lowerLine.startsWith('disallow:')) {
                        const pattern = line.substring(9).trim();
                        if (pattern) { // Kh√¥ng th√™m empty disallow rules
                            rules.push({ type: 'disallow', pattern });
                        }
                    } else if (lowerLine.startsWith('allow:')) {
                        const pattern = line.substring(6).trim();
                        if (pattern) {
                            rules.push({ type: 'allow', pattern });
                        }
                    }
                }
            }
            
            // √Åp d·ª•ng rules theo th·ª© t·ª± - rules sau s·∫Ω override rules tr∆∞·ªõc
            let allowed = true; // M·∫∑c ƒë·ªãnh cho ph√©p
            
            for (const rule of rules) {
                if (this.matchesPattern(fullUrl, urlPath, rule.pattern)) {
                    allowed = rule.type === 'allow';
                    apiLogger.debug(`URL ${url} kh·ªõp rule: ${rule.type} ${rule.pattern} -> ${allowed}`);
                }
            }
            
            return allowed;
            
        } catch (error) {
            apiLogger.warn(`L·ªói trong custom robots.txt parsing cho ${url}:`, { error });
            return true; // Cho ph√©p khi c√≥ l·ªói
        }
    }

    /**
     * Ki·ªÉm tra xem URL c√≥ kh·ªõp v·ªõi robots.txt pattern kh√¥ng
     * @param fullUrl - URL ƒë·∫ßy ƒë·ªß v·ªõi query parameters
     * @param urlPath - Ch·ªâ pathname c·ªßa URL
     * @param pattern - Pattern t·ª´ robots.txt (Allow/Disallow)
     * @returns true n·∫øu URL kh·ªõp v·ªõi pattern
     */
    private matchesPattern(fullUrl: string, urlPath: string, pattern: string): boolean {
        // X·ª≠ l√Ω wildcard patterns (c√≥ ch·ª©a *)
        if (pattern.includes('*')) {
            // Convert robots.txt pattern th√†nh regex
            const regexPattern = pattern
                .replace(/\*/g, '.*')
                .replace(/\?/g, '\\?')
                .replace(/\$/g, '$');
            
            try {
                const regex = new RegExp('^' + regexPattern);
                return regex.test(urlPath) || regex.test(fullUrl);
            } catch (error) {
                // Fallback sang simple string matching n·∫øu regex fail
                return urlPath.startsWith(pattern.replace('*', ''));
            }
        }
        
        // X·ª≠ l√Ω exact path matching (pattern k·∫øt th√∫c b·∫±ng /)
        if (pattern.endsWith('/')) {
            return urlPath.startsWith(pattern);
        }
        
        // X·ª≠ l√Ω query parameter patterns nh∆∞ /*?p=
        if (pattern.includes('?')) {
            return fullUrl.includes(pattern.replace('*', ''));
        }
        
        // Exact match ho·∫∑c starts with pattern
        return urlPath === pattern || urlPath.startsWith(pattern);
    }

    /**
     * L·∫•y crawl delay ph√π h·ª£p cho URL
     * Ki·ªÉm tra robots.txt tr∆∞·ªõc, fallback sang random delay
     * @param url - URL c·∫ßn crawl
     * @returns Delay time in milliseconds
     */
    async getCrawlDelay(url: string): Promise<number> {
        try {
            const origin = new URL(url).origin;
            const robotsTxt = await this.getRobotsTxt(origin);
            
            if (!robotsTxt) {
                return this.getRandomDelay();
            }

            // Parse crawl-delay manually v√¨ Crawlee kh√¥ng expose getCrawlDelay method
            const crawlDelay = this.parseCrawlDelay(robotsTxt, this.userAgent);
            if (crawlDelay && crawlDelay > 0) {
                const delayMs = crawlDelay * 1000;
                apiLogger.info(`S·ª≠ d·ª•ng robots.txt crawl-delay: ${delayMs}ms cho ${origin}`);
                return delayMs;
            }

            return this.getRandomDelay();
        } catch (error) {
            apiLogger.warn('L·ªói khi l·∫•y crawl delay:', { error });
            return this.getRandomDelay();
        }
    }

    /**
     * T·∫°o random delay trong kho·∫£ng minDelay - maxDelay
     * @returns Random delay in milliseconds
     */
    private getRandomDelay(): number {
        const delay = Math.floor(Math.random() * (this.maxDelay - this.minDelay + 1)) + this.minDelay;
        apiLogger.info(`S·ª≠ d·ª•ng random delay: ${delay}ms`);
        return delay;
    }

    /**
     * Parse crawl-delay t·ª´ robots.txt content
     * @param robotsTxt - RobotsTxtFile object
     * @param userAgent - User agent ƒë·ªÉ match
     * @returns Crawl delay in seconds, ho·∫∑c null n·∫øu kh√¥ng t√¨m th·∫•y
     */
    private parseCrawlDelay(robotsTxt: RobotsTxtFile, userAgent: string): number | null {
        try {
            // Access raw content t·ª´ robots.txt
            const content = (robotsTxt as any).content || '';
            const lines = content.split('\n');
            let inUserAgentSection = false;
            let crawlDelay: number | null = null;

            for (const line of lines) {
                const trimmedLine = line.trim().toLowerCase();
                
                // Ki·ªÉm tra user-agent section
                if (trimmedLine.startsWith('user-agent:')) {
                    const ua = trimmedLine.split(':')[1]?.trim();
                    inUserAgentSection = ua === '*' || ua === userAgent.toLowerCase();
                }
                
                // Parse crawl-delay n·∫øu trong ƒë√∫ng user-agent section
                if (inUserAgentSection && trimmedLine.startsWith('crawl-delay:')) {
                    const delay = parseFloat(trimmedLine.split(':')[1]?.trim() || '0');
                    if (!isNaN(delay)) {
                        crawlDelay = delay;
                    }
                }
            }

            return crawlDelay;
        } catch (error) {
            apiLogger.warn('L·ªói khi parse crawl-delay t·ª´ robots.txt:', { error });
            return null;
        }
    }

    /**
     * L·∫•y headers ph√π h·ª£p cho HTTP requests
     * @returns Object ch·ª©a User-Agent v√† Accept-Language headers
     */
    getHeaders(): Record<string, string> {
        return {
            'User-Agent': this.userAgent,
            'Accept-Language': this.acceptLanguage,
        };
    }

    /**
     * X·ª≠ l√Ω HTTP status codes v√† t√≠nh to√°n backoff delay ph√π h·ª£p
     * @param statusCode - HTTP status code nh·∫≠n ƒë∆∞·ª£c
     * @param url - URL ƒëang crawl
     * @returns Delay time trong milliseconds
     */
    async handleHttpStatus(statusCode: number, url: string): Promise<number> {
        const baseDelay = this.getRandomDelay();
        
        switch (statusCode) {
            case 429: // Too Many Requests - Rate limiting
                const backoffDelay = baseDelay * 3;
                apiLogger.warn(`HTTP 429 (Qu√° nhi·ªÅu requests) cho ${url}, backoff ${backoffDelay}ms`);
                return backoffDelay;
                
            case 503: // Service Unavailable - Server qu√° t·∫£i
                const serviceDelay = baseDelay * 2;
                apiLogger.warn(`HTTP 503 (Service Unavailable) cho ${url}, ch·ªù ${serviceDelay}ms`);
                return serviceDelay;
                
            case 502: // Bad Gateway
            case 504: // Gateway Timeout
                const gatewayDelay = baseDelay * 1.5;
                apiLogger.warn(`HTTP ${statusCode} (Gateway Error) cho ${url}, ch·ªù ${gatewayDelay}ms`);
                return gatewayDelay;
                
            default:
                // Cho c√°c status codes kh√°c, s·ª≠ d·ª•ng normal delay
                return baseDelay;
        }
    }

    /**
     * Ki·ªÉm tra xem c√≥ n√™n retry tr√™n status code n√†y kh√¥ng
     * @param statusCode - HTTP status code
     * @returns true n·∫øu n√™n retry, false n·∫øu kh√¥ng
     */
    shouldRetryOnStatus(statusCode: number): boolean {
        // Retry tr√™n server errors v√† rate limiting
        return [429, 502, 503, 504].includes(statusCode);
    }
}