/**
 * SunWest Genetics Product List Scraper (Cheerio - Standard Pagination)
 * 
 * Uses Cheerio for fast HTML parsing with standard pagination
 * Adapted from Vancouver Seed Bank scraper
 */

import { extractProductsFromHTML } from '@/scrapers/sunwestgenetics/utils/extractProductsFromHTML';
import { ProductsDataResultFromCrawling, ProductCardDataFromCrawling } from '@/types/crawl.type';
import { CheerioCrawler, Dataset, RequestQueue } from 'crawlee';
import { SiteConfig } from '@/lib/factories/scraper-factory';
import { apiLogger } from '@/lib/helpers/api-logger';
import { SimplePoliteCrawler } from '@/lib/utils/polite-crawler';
import { ACCEPTLANGUAGE, USERAGENT } from '@/scrapers/(common)/constants';

/**
 * SunWestGeneticsProductListScraper
 * 
 * Nhiá»‡m vá»¥ chÃ­nh:
 * 1. Crawl danh sÃ¡ch sáº£n pháº©m tá»« SunWest Genetics (product listing pages)
 * 2. Há»— trá»£ 2 cháº¿ Ä‘á»™:
 *    - Fixed mode: Crawl sá»‘ trang cá»‘ Ä‘á»‹nh (maxPages > 0)
 *    - Auto mode: Crawl tá»± Ä‘á»™ng Ä‘áº¿n háº¿t trang (maxPages = 0)
 * 
 * 3. Extract thÃ´ng tin tá»« product sections:
 *    - TÃªn sáº£n pháº©m, URL, slug
 *    - HÃ¬nh áº£nh
 *    - Strain type (Indica Dominant Hybrid, Sativa Dominant Hybrid, Balanced Hybrid, etc.)
 *    - Rating vÃ  review count tá»« "Rated X.XX out of 5 based on N customer ratings"
 *    - THC/CBD levels vá»›i parsing thÃ´ng minh tá»« text patterns
 *    - Flowering time, growing level
 *    - Pricing vá»›i pack sizes (5, 10, 25 seeds) tá»« "Pack 5 10 25 $65.00 â€“ $240.00"
 * 
 * 4. Sá»­ dá»¥ng CheerioCrawler:
 *    - Parse HTML sections thay vÃ¬ structured product cards
 *    - Text-based extraction cho SunWest Genetics format
 * 
 * 5. Tráº£ vá» CategoryResultFromCrawling:
 *    - Danh sÃ¡ch products[]
 *    - Metadata (totalProducts, totalPages, duration)
 * 
 * LÆ°u Ã½:
 * - KhÃ´ng lÆ°u database, chá»‰ crawl vÃ  return data
 * - Äá»ƒ lÆ°u DB, dÃ¹ng SaveDbService
 * - Äá»ƒ crawl theo batch, dÃ¹ng scrape-batch.ts script
 * 
 * SunWestGeneticsProductListScraper
    â”‚
    â”œâ”€> Fetch page 1, 2, 3... (CheerioCrawler)
    â”‚
    â””â”€> Má»—i page gá»i extractProductsFromHTML($)
            â”‚
            â””â”€> Parse HTML sections â†’ extract text patterns â†’ return products[]
 */

/**
 * SunWest Genetics Product List Scraper function
 * 
 * Follows the same pattern as Vancouver Seed Bank scraper:
 * - Takes SiteConfig as parameter
 * - Auto-detects total pages from first page crawl
 * - Returns ProductsDataResultFromCrawling
 */
export async function sunwestgeneticsScraper(
    siteConfig: SiteConfig, 
    dbMaxPage?: number,
    startPage: number = 1,
    endPage?: number
): Promise<ProductsDataResultFromCrawling> {
    const startTime = Date.now();
    
    const { selectors, baseUrl } = siteConfig;
    // Debug log Ä‘á»ƒ kiá»ƒm tra siteConfig
    apiLogger.info('[SunWest Product List] Starting with siteConfig', {
        name: siteConfig.name,
        baseUrl: siteConfig.baseUrl,
        isImplemented: siteConfig.isImplemented,
        startPage,
        endPage
    });

    const runId = Date.now();
    const datasetName = `sunwest-${runId}`;
    const dataset = await Dataset.open(datasetName);
    const requestQueue = await RequestQueue.open(`sunwest-queue-${runId}`);

    let actualPages = 0;
    const emptyPages = new Set<string>();

    // âœ… STEP 0: Initialize SimplePoliteCrawler and parse robots.txt FIRST
    const politeCrawler = new SimplePoliteCrawler({
        userAgent: USERAGENT,
        acceptLanguage: ACCEPTLANGUAGE
    });

    // âœ… Parse robots.txt at initialization
    const robotsRules = await politeCrawler.parseRobots(baseUrl);
    const { crawlDelay, disallowedPaths, allowedPaths, hasExplicitCrawlDelay } = robotsRules;

    apiLogger.info('[SunWest Product List] ğŸ¤– Robots.txt compliance', {
        crawlDelay: `${crawlDelay}ms`,
        hasExplicitCrawlDelay,
        disallowedPaths: disallowedPaths.length,
        allowedPaths: allowedPaths.length,
        strategy: hasExplicitCrawlDelay ? 'robots.txt enforced' : 'intelligent default'
    });

    // âœ… STEP 1: Calculate dynamic rate limiting based on robots.txt crawlDelay
    const calculatedMaxRate = hasExplicitCrawlDelay 
        ? Math.floor(60000 / crawlDelay)  // Respect robots.txt
        : 15;                              // Intelligent default

    const maxConcurrency = 1; // Sequential for same site

    apiLogger.info('[SunWest Product List] âš™ï¸ Crawler configuration', {
        crawlDelayMs: crawlDelay,
        maxRequestsPerMinute: calculatedMaxRate,
        maxConcurrency,
        hasExplicitCrawlDelay,
        mode: endPage ? 'TEST' : 'AUTO'
    });


    const crawler = new CheerioCrawler({
        requestQueue,
        async requestHandler({ $, request, log }) {
            log.info(`[SunWest Product List] Scraping: ${request.url}`);

            // Extract products and pagination from current page
            const extractResult = extractProductsFromHTML($, selectors, baseUrl, dbMaxPage);
            const products = extractResult.products;
            const maxPages = extractResult.maxPages;
            
            log.info(`[SunWest Product List] Extracted ${products.length} products`);
            if (maxPages) {
                log.info(`[SunWest Product List] Detected ${maxPages} total pages from pagination`);
            }

            // Track empty pages
            if (products.length === 0) {
                emptyPages.add(request.url);
            }

            // Check if there's a next page
            const hasNextPage = $(selectors.nextPage).length > 0;
            log.info(`[SunWest Product List] Has next page: ${hasNextPage}`);

            await dataset.pushData({ 
                products, 
                url: request.url, 
                hasNextPage,
                maxPages: maxPages // Include maxPages in dataset
            });

            // âœ… POLITE CRAWLING: Apply delay from parsed robots.txt
            log.debug(`[SunWest Product List] â±ï¸ Applying crawl delay: ${crawlDelay}ms (${hasExplicitCrawlDelay ? 'robots.txt' : 'default'})`);
            await new Promise(resolve => setTimeout(resolve, crawlDelay));
        },

        maxRequestsPerMinute: calculatedMaxRate, // âœ… Dynamic rate from robots.txt
        maxConcurrency: maxConcurrency,          // âœ… Sequential requests
        maxRequestRetries: 3,
    });

    // Auto-crawl mode: Start with startPage to detect maxPages, then crawl remaining pages
    apiLogger.info(`[SunWest Product List] Starting crawl with page ${startPage} to detect pagination...`);
        
    // First, crawl startPage to detect maxPages from pagination  
    const firstPageUrl = startPage === 1 ? 
        `${baseUrl}/shop/` : 
        `${baseUrl}/shop/page/${startPage}/`;
    
    await requestQueue.addRequest({ url: firstPageUrl });
    await crawler.run();
    
    // Check first page result to get maxPages and products
    const firstResults = await dataset.getData();
    let detectedMaxPages = startPage; // default fallback
    
    if (firstResults.items.length > 0) {
        const firstResult = firstResults.items[0] as any;
        if (firstResult.products && firstResult.products.length > 0) {
            apiLogger.info(`[SunWest Product List] Found ${firstResult.products.length} products on page ${startPage}`);
            
            // Try to detect pagination from extractProductsFromHTML
            detectedMaxPages = firstResult.maxPages || startPage;
            apiLogger.info(`[SunWest Product List] Detected ${detectedMaxPages} total pages from pagination`);
            
            // Calculate effective end page
            const effectiveEndPage = endPage ? Math.min(endPage, detectedMaxPages) : detectedMaxPages;
            
            // Now crawl remaining pages (startPage+1 to effectiveEndPage) if needed
            if (effectiveEndPage > startPage) {
                const remainingUrls: string[] = [];
                
                for (let page = startPage + 1; page <= effectiveEndPage; page++) {
                    // SunWest Genetics WooCommerce standard format: /shop/page/2/
                    remainingUrls.push(`${baseUrl}/shop/page/${page}/`);
                }
                
                if (remainingUrls.length > 0) {
                    apiLogger.info(`[SunWest Product List] Crawling remaining ${remainingUrls.length} pages (${startPage + 1} to ${effectiveEndPage})...`);
                    for (const url of remainingUrls) {
                        await requestQueue.addRequest({ url });
                    }
                    await crawler.run();
                }
            }
            
            actualPages = effectiveEndPage - startPage + 1;
        } else {
            apiLogger.warn(`[SunWest Product List] No products found on page ${startPage}, using fallback`);
        }
    } else {
        apiLogger.warn(`[SunWest Product List] No results from page ${startPage} crawl`);
    }

    // Collect results from dataset
    const results = await dataset.getData();
    const allProducts: ProductCardDataFromCrawling[] = [];

    results.items.forEach((item) => {
        allProducts.push(...(item as { products: ProductCardDataFromCrawling[] }).products);
    });

    const duration = Date.now() - startTime;

    // âœ… Aggregated logging
    apiLogger.info('[SunWest Product List] âœ… Crawling completed', {
        'ğŸ“Š Products': allProducts.length,
        'ğŸ“„ Pages': actualPages,
        'â±ï¸ Duration': `${(duration / 1000).toFixed(2)}s`,
        'ğŸ¤– Robots.txt': hasExplicitCrawlDelay ? 'enforced' : 'default',
        'ğŸš€ Rate': `${calculatedMaxRate} req/min`
    });

    return {
        totalProducts: allProducts.length,
        totalPages: actualPages,
        products: allProducts,
        timestamp: new Date(),
        duration,
    };
}