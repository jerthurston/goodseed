/**
 * Vancouver Seed Bank Product List Scraper (Cheerio - Standard Pagination)
 * 
 * KI·∫æN TR√öC T·ªîNG QUAN:
 * - Uses Cheerio for fast HTML parsing v·ªõi WooCommerce standard pagination
 * - Design Pattern: Function-based scraper v·ªõi delegation pattern
 * - Performance: Cheerio nhanh h∆°n 10-20x so v·ªõi Playwright browser automation
 * - Rate Limiting: Tu√¢n th·ªß 2-5 gi√¢y delay gi·ªØa requests theo project requirement
 */

import { extractProductsFromHTML } from '@/scrapers/vancouverseedbank/utils/extractProductsFromHTML';
import { ProductsDataResultFromCrawling, ProductCardDataFromCrawling } from '@/types/crawl.type';
import { CheerioCrawler, Dataset, RequestQueue, RobotsTxtFile } from 'crawlee';
import { SiteConfig } from '@/lib/factories/scraper-factory';
import { apiLogger } from '@/lib/helpers/api-logger';
import { SimplePoliteCrawler } from '@/lib/utils/polite-crawler';

/**
 * ProductListScraper - LU·ªíNG X·ª¨ L√ù CH√çNH
 * 
 * NHI·ªÜM V·ª§ CH√çNH:
 * 1. üï∑Ô∏è Crawl danh s√°ch s·∫£n ph·∫©m t·ª´ Vancouver Seed Bank (product listing pages)
 * 2. üìÑ H·ªó tr·ª£ ch·∫ø ƒë·ªô:
 *    - Auto mode: Crawl t·ª± ƒë·ªông ƒë·∫øn h·∫øt trang (maxPages = 0)
 *    - Limited mode: Crawl v·ªõi gi·ªõi h·∫°n dbMaxPage parameter
 * 
 * 3. üìã Extract th√¥ng tin t·ª´ product cards:
 *    - T√™n s·∫£n ph·∫©m, URL, slug
 *    - H√¨nh ·∫£nh (x·ª≠ l√Ω lazy loading v·ªõi data-src fallback)
 *    - Strain type (Indica, Sativa, Hybrid)
 *    - Rating v√† review count
 *    - THC/CBD levels (min/max parsing)
 *    - Flowering time, growing level
 * 
 * 4. ‚ö° S·ª≠ d·ª•ng CheerioCrawler (nhanh, kh√¥ng c·∫ßn Playwright):
 *    - Ph√π h·ª£p v·ªõi WooCommerce standard pagination (kh√¥ng c√≥ JS dynamic content)
 *    - Kh√¥ng c·∫ßn browser rendering ‚Üí ti·∫øt ki·ªám resources
 *    - Sequential crawling v·ªõi rate limiting
 * 
 * 5. üì§ Tr·∫£ v·ªÅ ProductsDataResultFromCrawling:
 *    - Danh s√°ch products[] v·ªõi full metadata
 *    - Pagination info (totalProducts, totalPages, duration)
 *    - Performance metrics cho monitoring
 * 
 * SEPARATION OF CONCERNS:
 * - Scraper n√†y KH√îNG l∆∞u database, ch·ªâ crawl v√† return data
 * - ƒê·ªÉ l∆∞u DB: d√πng VancouverSeedBankDbService
 * - ƒê·ªÉ crawl theo batch: d√πng scrape-batch.ts script
 * 
 * LU·ªíNG D·ªÆ LI·ªÜU:
 * VancouverSeedBankProductListScraper
    ‚îÇ
    ‚îú‚îÄ> üåê Fetch page 1, 2, 3... (CheerioCrawler v·ªõi rate limiting)
    ‚îÇ
    ‚îî‚îÄ> üìÑ M·ªói page g·ªçi extractProductsFromHTML($) 
            ‚îÇ
            ‚îî‚îÄ> üîç Parse HTML ‚Üí return products[] v·ªõi full metadata

    /**
     * MAIN SCRAPER FUNCTION - Entry point cho Vancouver Seed Bank scraping
     * 
     * PARAMETERS EXPLAINED:
     * @param siteConfig - Factory pattern config ch·ª©a baseUrl, selectors, implementation status
     * @param dbMaxPage - GI·ªöI H·∫†N PAGES: undefined = unlimited, number = max pages to crawl
     * @param startPage - Start t·ª´ page n√†o (currently not implemented - future enhancement)
     * @param endPage - End ·ªü page n√†o (currently not implemented - future enhancement) 
     * @param fullSiteCrawl - Full site mode vs limited mode (currently not implemented)
     * 
     * RETURN: ProductsDataResultFromCrawling v·ªõi complete metadata
     */

        /**
         * Polite crawling should be happen like that:
         * 1. Tr√¨nh t·ª± t·ªïng th·ªÉ (High-level flow)
        Start
        ‚Üì
        Check Legal / ToS / robots.txt
        ‚Üì
        Decide Crawl Scope
        ‚Üì
        Request Scheduling (Rate limit)
        ‚Üì
        Fetch Page (with headers)
        ‚Üì
        Respect Response (status / retry / backoff)
        ‚Üì
        Parse & Extract Data
        ‚Üì
        Normalize & Store
        ‚Üì
        Cache / Fingerprint
        ‚Üì
        Schedule Next Crawl
        ‚Üì
        End
        */

export async function vancouverProductListScraper(
    siteConfig: SiteConfig,
    // dbMaxPage?: number,
    startPage?: number | null,
    endPage?: number | null,
    fullSiteCrawl?: boolean | null,
    sourceContext?: {
        scrapingSourceUrl: string;
        sourceName: string;
        dbMaxPage: number;
    }
): Promise<ProductsDataResultFromCrawling> {
    const startTime = Date.now();

    const { baseUrl, selectors } = siteConfig


    // Debug log ƒë·ªÉ ki·ªÉm tra siteConfig
    apiLogger.info('[Product List] Starting with siteConfig', {
        name: siteConfig.name,
        baseUrl: siteConfig.baseUrl,
        isImplemented: siteConfig.isImplemented
    });

    const runId = Date.now();
    const datasetName = `vsb-${runId}`;
    const dataset = await Dataset.open(datasetName);
    const requestQueue = await RequestQueue.open(`vsb-queue-${runId}`);

    // Initialize polite crawler
    const politeCrawler = new SimplePoliteCrawler({
        userAgent: 'GoodSeed-Bot/1.0 (+https://goodseed.ca/contact) Commercial Cannabis Research',
        acceptLanguage: 'en-US,en;q=0.9',
        minDelay: 2000,
        maxDelay: 5000
    });

    let actualPages = 0;
    const emptyPages = new Set<string>();

    // X·ª≠ l√Ω load robots.txt
    // const robots = await RobotsTxtFile.find('https://vancouverseedbank.ca/robots.txt')
    // apiLogger.info('[Product List] Loaded robots.txt', { robots });


    const crawler = new CheerioCrawler({
        requestQueue,
        async requestHandler({ $, request, log }) {
            log.info(`[Product List] Scraping: ${request.url}`);

            // POLITE CRAWLING: Check robots.txt compliance
            const isAllowed = await politeCrawler.isAllowed(request.url);
            if (!isAllowed) {
                log.error(`[Product List] BLOCKED by robots.txt: ${request.url}`);
                throw new Error(`robots.txt blocked access to ${request.url}`);
            }

            // Extract products and pagination from current page
            const extractResult = extractProductsFromHTML($, siteConfig, sourceContext?.dbMaxPage, startPage, endPage, fullSiteCrawl);
            const products = extractResult.products;
            const maxPages = extractResult.maxPages;

            log.info(`[Product List] Extracted ${products.length} products`);
            if (maxPages) {
                log.info(`[Product List] Detected ${maxPages} total pages from pagination`);
            }

            // Track empty pages
            if (products.length === 0) {
                emptyPages.add(request.url);
            }

            // Check if there's a next page
            const hasNextPage = $(selectors.nextPage).length > 0;
            log.info(`[Product List] Has next page: ${hasNextPage}`);

            await dataset.pushData({
                products,
                url: request.url,
                hasNextPage,
                maxPages: maxPages // Include maxPages in dataset
            });

            // POLITE CRAWLING: Use polite crawler for delays and robots.txt compliance
            const delayMs = await politeCrawler.getCrawlDelay(request.url);
            log.info(`[Product List] Using polite crawl delay: ${delayMs}ms for ${request.url}`);
            await new Promise(resolve => setTimeout(resolve, delayMs));
        },
        maxRequestsPerMinute: 15, // Reduced to ensure 2-5 second delays are respected
        maxConcurrency: 1, // Sequential requests within same site (project requirement)
        maxRequestRetries: 3,
        preNavigationHooks: [
            async (crawlingContext, requestAsBrowserOptions) => {
                // Add polite crawler headers
                const headers = politeCrawler.getHeaders();
                Object.assign(requestAsBrowserOptions.headers || {}, headers);
            }
        ],
        errorHandler: async ({ request, error, log }) => {
            // POLITE CRAWLING: Handle HTTP status codes properly
            const httpError = error as any;
            if (httpError?.response?.status) {
                const statusCode = httpError.response.status;
                const shouldRetry = politeCrawler.shouldRetryOnStatus(statusCode);
                
                if (shouldRetry) {
                    const backoffDelay = await politeCrawler.handleHttpStatus(statusCode, request.url);
                    log.info(`[Product List] HTTP ${statusCode} for ${request.url}, backing off for ${backoffDelay}ms`);
                    await new Promise(resolve => setTimeout(resolve, backoffDelay));
                    throw error; // Re-throw to trigger retry
                } else {
                    log.error(`[Product List] Non-retryable HTTP ${statusCode} for ${request.url}`);
                    throw error;
                }
            } else {
                log.error(`[Product List] Non-HTTP error for ${request.url}`);
                throw error;
            }
        },
    });

    // Auto-crawl mode: Start v·ªõi page 1 ƒë·ªÉ detect maxPages, sau ƒë√≥ crawl remaining pages
    apiLogger.info('[Product List] Starting crawl with page 1 to detect pagination...');

    // Extract path from source URL - handle both /shop and /product-category/* patterns
    let sourcePath = '/shop'; // default fallback

    if (sourceContext?.scrapingSourceUrl) {
        try {
            const url = new URL(sourceContext.scrapingSourceUrl);
            sourcePath = url.pathname;

            // Ensure path doesn't end with slash for consistent URL building
            sourcePath = sourcePath.replace(/\/$/, '');

            apiLogger.info(`[Product List] Using dynamic source path: ${sourcePath}`);
        } catch (error) {
            apiLogger.warn('[Product List] Invalid sourceContext URL, using default /shop', {
                url: sourceContext.scrapingSourceUrl,
                error: error instanceof Error ? error.message : 'Unknown error'
            });
            sourcePath = '/shop';
        }
    } else {
        apiLogger.info('[Product List] No sourceContext provided, using default /shop');
    }

    const firstPageUrl = `${baseUrl}${sourcePath}/`;

    // First, crawl page 1 to detect maxPages from pagination
    // const firstPageUrl = `${baseUrl}${sourcePath}/`; 
    await requestQueue.addRequest({ url: firstPageUrl });
    await crawler.run();

    // Check first page result to get maxPages and products
    const firstResults = await dataset.getData();
    let detectedMaxPages = 1; // default fallback

    if (firstResults.items.length > 0) {
        const firstResult = firstResults.items[0] as any;
        if (firstResult.products && firstResult.products.length > 0) {
            apiLogger.info(`[Product List] Found ${firstResult.products.length} products on page 1`);

            // Try to detect pagination from extractProductsFromHTML
            detectedMaxPages = firstResult.maxPages || 1;
            apiLogger.info(`[Product List] Detected ${detectedMaxPages} total pages from pagination`);

            // Now crawl remaining pages (2 to maxPages) if more than 1 page
            if (detectedMaxPages > 1) {
                const remainingUrls: string[] = [];

                // Use maxPages from test mode if available, otherwise use detected pages with safety limit
                const finalMaxPages = firstResult.maxPages || Math.min(detectedMaxPages, 50);

                for (let page = 2; page <= finalMaxPages; page++) {
                    const pageUrl = `${baseUrl}${sourcePath}/page/${page}/`;
                    remainingUrls.push(pageUrl);
                }

                if (remainingUrls.length > 0) {
                    apiLogger.info(`[Product List] Crawling remaining ${remainingUrls.length} pages (finalMaxPages=${finalMaxPages})...`);
                    for (const url of remainingUrls) {
                        await requestQueue.addRequest({ url });
                    }
                    await crawler.run();
                }
            }
            // Use maxPages from test mode if available, otherwise use detected pages with safety limit
            actualPages = firstResult.maxPages || Math.min(detectedMaxPages, 50);
        } else {
            apiLogger.warn('[Product List] No products found on page 1, using fallback');
        }
    } else {
        apiLogger.warn('[Product List] No results from page 1 crawl');
    }

    // Collect results from dataset
    const results = await dataset.getData();
    const allProducts: ProductCardDataFromCrawling[] = [];

    results.items.forEach((item) => {
        allProducts.push(...(item as { products: ProductCardDataFromCrawling[] }).products);
    });

    return {
        // category: listingUrl,
        totalProducts: allProducts.length,
        totalPages: actualPages,
        products: allProducts,
        timestamp: new Date(),
        duration: Date.now() - startTime,
    };
}

// }


