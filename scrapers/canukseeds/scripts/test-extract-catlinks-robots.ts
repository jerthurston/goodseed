import { SimplePoliteCrawler } from '@/lib/utils/polite-crawler';
import { ACCEPTLANGUAGE, USERAGENT } from '@/scrapers/(common)/constants';
import { extractCategoryLinksFromHomepage } from '../utils/extractCatLinkFromHeader';

/**
 * Test extractCategoryLinksFromHomepage v·ªõi robots.txt compliance
 */
async function testExtractCatLinksWithRobots() {
    console.log('üß™ Testing extractCategoryLinksFromHomepage with robots.txt compliance...\n');
    
    const baseUrl = 'https://www.canukseeds.com';
    
    // Setup polite crawler
    const politeCrawler = new SimplePoliteCrawler({
        userAgent: USERAGENT,
        acceptLanguage: ACCEPTLANGUAGE,
        minDelay: 2000,
        maxDelay: 5000
    });
    
    try {
        console.log('üìã Step 1: Parse robots.txt rules');
        const robotsRules = await politeCrawler.parseRobots(baseUrl);
        console.log(`‚úÖ Parsed robots.txt - crawl delay: ${robotsRules.crawlDelay}ms`);
        
        console.log('\nüåê Step 2: Extract category links v·ªõi robots.txt compliance');
        const startTime = Date.now();
        
        const categoryLinks = await extractCategoryLinksFromHomepage(baseUrl, robotsRules);
        
        const endTime = Date.now();
        const duration = endTime - startTime;
        
        console.log('\nüìä RESULTS:');
        console.log('='.repeat(60));
        console.log(`‚úÖ Total category links extracted: ${categoryLinks.length}`);
        console.log(`‚è±Ô∏è Extraction duration: ${duration}ms`);
        console.log(`ü§ñ User-Agent used: ${robotsRules.userAgent}`);
        
        console.log('\nüìã Sample category links:');
        categoryLinks.slice(0, 10).forEach((link, index) => {
            console.log(`   ${index + 1}. ${link}`);
        });
        
        if (categoryLinks.length > 10) {
            console.log(`   ... v√† ${categoryLinks.length - 10} links kh√°c`);
        }
        
        // Verify robots.txt compliance
        console.log('\nüîç ROBOTS.TXT COMPLIANCE VERIFICATION:');
        console.log('='.repeat(60));
        
        let blockedCount = 0;
        let allowedCount = 0;
        
        for (const link of categoryLinks) {
            const linkPath = new URL(link).pathname;
            
            // Ki·ªÉm tra v·ªõi disallowed paths
            const isBlocked = robotsRules.disallowedPaths.some(disallowedPath => {
                return linkPath === disallowedPath || linkPath.startsWith(disallowedPath);
            });
            
            if (isBlocked) {
                blockedCount++;
                console.log(`‚ùå BLOCKED link found: ${link}`);
            } else {
                allowedCount++;
            }
        }
        
        console.log(`‚úÖ Allowed links: ${allowedCount}`);
        console.log(`‚ùå Blocked links: ${blockedCount}`);
        
        if (blockedCount === 0) {
            console.log('üéâ PERFECT! All extracted links comply with robots.txt');
        } else {
            console.log('‚ö†Ô∏è WARNING: Some links violate robots.txt rules');
        }
        
        // Test v·ªõi robots.txt delay
        if (robotsRules.crawlDelay > 0) {
            console.log(`\n‚è±Ô∏è Verifying crawl delay compliance: ${robotsRules.crawlDelay}ms`);
            console.log('   Expected: Function should apply delay before crawling');
            console.log('   Actual: Check logs above for "Applying robots.txt crawl delay"');
        }
        
        console.log('\nüéØ Test completed successfully!');
        
    } catch (error) {
        console.error('‚ùå Error testing extractCategoryLinksFromHomepage:', error);
    }
}

testExtractCatLinksWithRobots();