# Canuk Seeds Scraper

## üéØ T·ªïng quan
Scraper ƒë·ªÉ extract d·ªØ li·ªáu s·∫£n ph·∫©m t·ª´ website Canuk Seeds (https://www.canukseeds.com) v·ªõi h·ªó tr·ª£ pagination v√† crawling to√†n di·ªán.

## üìã Flow x·ª≠ l√Ω ch√≠nh

### **Step 0: Parse Robots.txt & Setup Polite Crawling**
```typescript
// Trong canukseedsScraper.ts
const politeCrawler = new SimplePoliteCrawler({
    userAgent: USERAGENT,
    acceptLanguage: ACCEPTLANGUAGE,
    minDelay: 2000, // Will be updated based on robots.txt
    maxDelay: 5000
});

// Parse robots.txt to get crawling rules
const robotsRules = await politeCrawler.parseRobots(baseUrl);
```
- **Input**: Base URL (`https://www.canukseeds.com`)
- **Process**: 
  - Parse robots.txt ƒë·ªÉ l·∫•y crawl delays, disallowed paths
  - Setup polite crawling parameters
  - Validate user-agent permissions
- **Output**: Crawling rules object ƒë·ªÉ pass cho c√°c extraction functions

### **Step 1: Extract Category Links t·ª´ Homepage**
```typescript
extractCategoryLinksFromHomepage(robotsRules)
```
- **Input**: Homepage URL + robots rules
- **Process**: Parse header navigation v·ªõi polite crawling
- **Output**: Array of 34 category URLs
  ```
  [
    "https://www.canukseeds.com/buy-canuk-seeds/standard-canuk-seeds",
    "https://www.canukseeds.com/buy-canuk-seeds/feminized-seeds",
    ...
  ]
  ```

### **Step 2: Extract Product URLs t·ª´ Categories v·ªõi Pagination**
```typescript
extractProductUrlsFromCatLink(categoryUrl, maxPages = 3, robotsRules)
```
- **Input**: Category URL + max pages + robots rules
- **Process**: 
  - Check robots.txt cho allowed paths
  - L·∫∑p qua t·ª´ng page v·ªõi proper delays
  - Extract product links tu√¢n theo crawl rate limits
- **Pagination Pattern**: `https://www.canukseeds.com/buy-canuk-seeds/feminized-seeds?p=2`
- **Output**: Array of product URLs per category (~36+ URLs per category)

### **Step 3: Process All Categories**
```typescript
// Trong canukseedsScraper.ts v·ªõi polite crawler
for (const catLink of catLinkArr) {
    // Validate URL against robots.txt
    if (politeCrawler.isAllowed(catLink, robotsRules)) {
        const productUrls = await extractProductUrlsFromCatLink(catLink, maxPages, robotsRules);
        urlsToProcess.push(...productUrls); // Remove duplicates
        
        // Apply robots.txt crawl delay
        await politeCrawler.delay(robotsRules.crawlDelay);
    }
}
```
- **Process**: L·∫∑p qua 34 categories v·ªõi robots.txt compliance
- **Safety**: Check disallowed paths, apply proper delays
- **Total Expected**: ~1000+ unique product URLs
- **Deduplication**: Remove duplicate URLs across categories

### **Step 4: Extract Product Data v·ªõi Polite Rules**
```typescript
extractProductFromDetailHTML($, siteConfig, productUrl, robotsRules)
```
- **Input**: Product detail URL + robots rules
- **Process**: Parse cannabis data v·ªõi respect cho robots.txt
- **Rate Limiting**: Apply crawl delays between product requests
- **Extracted Data**:
  - ‚úÖ Name (with ELITE STRAIN formatting)
  - ‚úÖ Seed Type (feminized/autoflower)
  - ‚úÖ Cannabis Type (indica/sativa/hybrid)
  - ‚úÖ THC Level (e.g., "19-24%")
  - ‚úÖ CBD Level (e.g., "2%")
  - ‚úÖ Flowering Time
  - ‚úÖ Pricing (multiple pack sizes)
  - ‚úÖ Image URL (OG meta fallback)

### **Step 5: Return Results**
```typescript
ProductsDataResultFromCrawling {
    products: ProductCardDataFromCrawling[],
    totalProducts: number,
    totalPages: number,
    timestamp: Date,
    duration: number
}
```

## üîß Infrastructure Components

### **Polite Crawling & Robots.txt Compliance**
- `SimplePoliteCrawler`: Central crawling manager v·ªõi robots.txt parsing
- `parseRobots()`: Parse robots.txt ƒë·ªÉ l·∫•y crawl delays v√† disallowed paths
- `isAllowed()`: Validate URLs against robots.txt rules
- `delay()`: Apply appropriate delays based on robots.txt

### **URL Management**
- `getScrapingUrl.ts`: Handles pagination URLs v·ªõi pattern `?p=N`
- Support functions: `getPageNumberFromUrl()`, `isPaginationUrl()`, `getScrapingUrlRange()`

### **Data Extraction v·ªõi Polite Rules**
- `selectors.ts`: WooCommerce selectors cho Canuk Seeds structure
- `extractProductFromDetailHTML.ts`: Parse cannabis product data v·ªõi robots.txt compliance
- `extractCatLinkFromHeader.ts`: Extract navigation links v·ªõi rate limiting

### **Quality Assurance**
- **Robots.txt Compliance**: Parse v√† follow t·∫•t c·∫£ robots.txt rules
- **Dynamic Rate Limiting**: Adjust delays based on site's robots.txt crawl-delay
- **Path Validation**: Check disallowed paths tr∆∞·ªõc khi crawl
- **Error Handling**: Continue on individual failures v·ªõi proper delays
- **Image Fallback**: OG meta tags khi Fotorama gallery fails
- **Deduplication**: Remove duplicate URLs v√† products

## üìä Performance Metrics

### **Tested Results v·ªõi Robots.txt Compliance**:
- ‚úÖ Robots.txt parsing: All URLs allowed
- ‚úÖ Dynamic crawl delays: 2.7s - 4.7s based on robots.txt
- ‚úÖ Category extraction: 34 categories
- ‚úÖ Single category: 36 product URLs
- ‚úÖ Product data extraction: 100% success rate (2/2 tested)
- ‚úÖ Pagination: Support multi-page crawling
- ‚úÖ Image extraction: OG fallback working
- ‚úÖ Polite crawling: Follows robots.txt rules completely

### **Estimated Scale**:
- Total categories: 34
- Pages per category: 3 (max)
- Products per page: ~36
- **Total products**: ~3,600 products potential

## üöÄ Entry Point
```typescript
canukSeedScraper(siteConfig, sourceContext, startPage?, endPage?)
```

## üéØ Status
- ‚úÖ **Robots.txt Compliance**: Parse v√† follow t·∫•t c·∫£ robots.txt rules
- ‚úÖ **URL Extraction**: Implemented v·ªõi pagination support
- ‚úÖ **Product Data**: High-quality extraction v·ªõi cannabis-specific fields  
- ‚úÖ **Polite Crawling**: Dynamic delays based on robots.txt (2.7s-4.7s)
- ‚úÖ **Error Handling**: Robust error handling v·ªõi path validation
- üîÑ **Ready for Production**: Full infrastructure completed v·ªõi ethical crawling

## üìù Ghi ch√∫ Implementation
- **Th·ª±c tr·∫°ng site**: Kh√¥ng c√≥ sitemap, ch·ªâ c√≥ robots.txt. Product cards ·ªü listings thi·∫øu th√¥ng tin.
- **Chi·∫øn l∆∞·ª£c**: Extract t·ª´ menu navigation ‚Üí pagination categories ‚Üí detail product pages
- **Architecture**: CommonCrawler pattern t∆∞∆°ng t·ª± maryjanesgarden v√† cropkingseeds
